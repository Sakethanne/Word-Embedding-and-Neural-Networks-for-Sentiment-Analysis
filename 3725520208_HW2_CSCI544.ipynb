{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Anne Sai Venkata Naga Saketh <br>\n",
    "USC ID: 3725520208 <br>\n",
    "USC Email: annes@usc.edu <br>\n",
    "<b> NLP HW2 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sakethanne/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sakethanne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import re            # Regular expressions for text processing\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk          # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet data\n",
    "nltk.download('stopwords')   # Download StopWords data\n",
    "\n",
    "import warnings      # To handle warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for the remainder of the code\n",
    "warnings.filterwarnings(\"default\")  # Set warnings back to default behavior\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakethanne/anaconda3/lib/python3.11/site-packages/session_info/main.py:213: DeprecationWarning: Accessing jsonschema.__version__ is deprecated and will be removed in a future release. Use importlib.metadata directly to query for jsonschema's version.\n",
      "  mod_version = _find_version(mod.__version__)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "bs4                 4.12.2\n",
       "gensim              4.3.0\n",
       "nltk                3.8.1\n",
       "numpy               1.24.3\n",
       "pandas              1.5.3\n",
       "session_info        1.0.0\n",
       "sklearn             1.2.2\n",
       "torch               2.0.1\n",
       "tqdm                4.65.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "CoreFoundation      NA\n",
       "Foundation          NA\n",
       "PyObjCTools         NA\n",
       "anyio               NA\n",
       "appnope             0.1.2\n",
       "asttokens           NA\n",
       "astunparse          1.6.3\n",
       "attr                22.1.0\n",
       "babel               2.11.0\n",
       "backcall            0.2.0\n",
       "beta_ufunc          NA\n",
       "binom_ufunc         NA\n",
       "bottleneck          1.3.5\n",
       "brotli              NA\n",
       "certifi             2023.07.22\n",
       "chardet             4.0.0\n",
       "charset_normalizer  2.0.4\n",
       "cloudpickle         2.2.1\n",
       "colorama            0.4.6\n",
       "comm                0.1.2\n",
       "cython_runtime      NA\n",
       "dateutil            2.8.2\n",
       "debugpy             1.6.7\n",
       "decorator           5.1.1\n",
       "dill                0.3.6\n",
       "entrypoints         0.4\n",
       "executing           0.8.3\n",
       "fastjsonschema      NA\n",
       "gmpy2               2.1.2\n",
       "google              NA\n",
       "hypergeom_ufunc     NA\n",
       "idna                3.4\n",
       "invgauss_ufunc      NA\n",
       "ipykernel           6.19.2\n",
       "ipython_genutils    0.2.0\n",
       "jedi                0.18.1\n",
       "jinja2              3.1.2\n",
       "joblib              1.2.0\n",
       "json5               NA\n",
       "jsonpointer         2.1\n",
       "jsonschema          4.17.3\n",
       "jupyter_server      1.23.4\n",
       "jupyterlab_server   2.22.0\n",
       "lxml                4.9.2\n",
       "lz4                 4.3.2\n",
       "markupsafe          2.1.1\n",
       "mpl_toolkits        NA\n",
       "mpmath              1.3.0\n",
       "nbformat            5.7.0\n",
       "nbinom_ufunc        NA\n",
       "ncf_ufunc           NA\n",
       "nct_ufunc           NA\n",
       "ncx2_ufunc          NA\n",
       "numexpr             2.8.4\n",
       "objc                9.0\n",
       "opt_einsum          v3.3.0\n",
       "packaging           23.0\n",
       "parso               0.8.3\n",
       "pexpect             4.8.0\n",
       "pickleshare         0.7.5\n",
       "pkg_resources       NA\n",
       "platformdirs        2.5.2\n",
       "prometheus_client   NA\n",
       "prompt_toolkit      3.0.36\n",
       "psutil              5.9.0\n",
       "ptyprocess          0.7.0\n",
       "pure_eval           0.2.2\n",
       "pvectorc            NA\n",
       "pyarrow             11.0.0\n",
       "pydev_ipython       NA\n",
       "pydevconsole        NA\n",
       "pydevd              2.9.5\n",
       "pydevd_file_utils   NA\n",
       "pydevd_plugins      NA\n",
       "pydevd_tracing      NA\n",
       "pygments            2.15.1\n",
       "pyrsistent          NA\n",
       "pytz                2022.7\n",
       "regex               2.5.116\n",
       "requests            2.31.0\n",
       "rfc3339_validator   0.1.4\n",
       "rfc3986_validator   0.1.1\n",
       "ruamel              NA\n",
       "scipy               1.10.1\n",
       "send2trash          NA\n",
       "setuptools          68.0.0\n",
       "six                 1.16.0\n",
       "skewnorm_ufunc      NA\n",
       "smart_open          5.2.1\n",
       "sniffio             1.3.0\n",
       "socks               1.7.1\n",
       "soupsieve           2.4\n",
       "sphinxcontrib       NA\n",
       "stack_data          0.2.0\n",
       "sympy               1.11.1\n",
       "terminado           0.17.1\n",
       "threadpoolctl       2.2.0\n",
       "tornado             6.3.2\n",
       "traitlets           5.7.1\n",
       "typing_extensions   NA\n",
       "urllib3             1.26.16\n",
       "wcwidth             0.2.5\n",
       "websocket           0.58.0\n",
       "zmq                 23.2.0\n",
       "zope                NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.12.0\n",
       "jupyter_client      7.4.9\n",
       "jupyter_core        5.3.0\n",
       "jupyterlab          3.6.3\n",
       "notebook            6.5.4\n",
       "-----\n",
       "Python 3.11.4 (main, Jul  5 2023, 08:54:11) [Clang 14.0.6 ]\n",
       "macOS-14.3.1-arm64-arm-64bit\n",
       "-----\n",
       "Session information updated at 2024-02-08 14:47\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Set Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the bad lines or the rows in the TSV file that contain in-correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/1150709939.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  full_data = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n",
      "Skipping line 20773: expected 15 fields, saw 22\n",
      "Skipping line 39834: expected 15 fields, saw 22\n",
      "Skipping line 52957: expected 15 fields, saw 22\n",
      "Skipping line 54540: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 80276: expected 15 fields, saw 22\n",
      "Skipping line 96168: expected 15 fields, saw 22\n",
      "Skipping line 96866: expected 15 fields, saw 22\n",
      "Skipping line 98175: expected 15 fields, saw 22\n",
      "Skipping line 112539: expected 15 fields, saw 22\n",
      "Skipping line 119377: expected 15 fields, saw 22\n",
      "Skipping line 120065: expected 15 fields, saw 22\n",
      "Skipping line 124703: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 134024: expected 15 fields, saw 22\n",
      "Skipping line 153938: expected 15 fields, saw 22\n",
      "Skipping line 156225: expected 15 fields, saw 22\n",
      "Skipping line 168603: expected 15 fields, saw 22\n",
      "Skipping line 187002: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 200397: expected 15 fields, saw 22\n",
      "Skipping line 203809: expected 15 fields, saw 22\n",
      "Skipping line 207680: expected 15 fields, saw 22\n",
      "Skipping line 223421: expected 15 fields, saw 22\n",
      "Skipping line 244032: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 270329: expected 15 fields, saw 22\n",
      "Skipping line 276484: expected 15 fields, saw 22\n",
      "Skipping line 304755: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 379449: expected 15 fields, saw 22\n",
      "Skipping line 386191: expected 15 fields, saw 22\n",
      "Skipping line 391811: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 414348: expected 15 fields, saw 22\n",
      "Skipping line 414773: expected 15 fields, saw 22\n",
      "Skipping line 417572: expected 15 fields, saw 22\n",
      "Skipping line 419496: expected 15 fields, saw 22\n",
      "Skipping line 430528: expected 15 fields, saw 22\n",
      "Skipping line 442230: expected 15 fields, saw 22\n",
      "Skipping line 450931: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 465377: expected 15 fields, saw 22\n",
      "Skipping line 467685: expected 15 fields, saw 22\n",
      "Skipping line 485055: expected 15 fields, saw 22\n",
      "Skipping line 487220: expected 15 fields, saw 22\n",
      "Skipping line 496076: expected 15 fields, saw 22\n",
      "Skipping line 512269: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 529505: expected 15 fields, saw 22\n",
      "Skipping line 531286: expected 15 fields, saw 22\n",
      "Skipping line 535424: expected 15 fields, saw 22\n",
      "Skipping line 569898: expected 15 fields, saw 22\n",
      "Skipping line 586293: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 593880: expected 15 fields, saw 22\n",
      "Skipping line 599274: expected 15 fields, saw 22\n",
      "Skipping line 607961: expected 15 fields, saw 22\n",
      "Skipping line 612413: expected 15 fields, saw 22\n",
      "Skipping line 615913: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 677580: expected 15 fields, saw 22\n",
      "Skipping line 687191: expected 15 fields, saw 22\n",
      "Skipping line 710819: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 728692: expected 15 fields, saw 22\n",
      "Skipping line 730216: expected 15 fields, saw 22\n",
      "Skipping line 758397: expected 15 fields, saw 22\n",
      "Skipping line 760061: expected 15 fields, saw 22\n",
      "Skipping line 768935: expected 15 fields, saw 22\n",
      "Skipping line 769483: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 822725: expected 15 fields, saw 22\n",
      "Skipping line 823621: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 857041: expected 15 fields, saw 22\n",
      "Skipping line 857320: expected 15 fields, saw 22\n",
      "Skipping line 858565: expected 15 fields, saw 22\n",
      "Skipping line 860629: expected 15 fields, saw 22\n",
      "Skipping line 864033: expected 15 fields, saw 22\n",
      "Skipping line 868673: expected 15 fields, saw 22\n",
      "Skipping line 869189: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 938605: expected 15 fields, saw 22\n",
      "Skipping line 940100: expected 15 fields, saw 22\n",
      "Skipping line 975137: expected 15 fields, saw 22\n",
      "Skipping line 976314: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 985597: expected 15 fields, saw 22\n",
      "Skipping line 990873: expected 15 fields, saw 22\n",
      "Skipping line 991806: expected 15 fields, saw 22\n",
      "Skipping line 1019808: expected 15 fields, saw 22\n",
      "Skipping line 1021526: expected 15 fields, saw 22\n",
      "Skipping line 1023905: expected 15 fields, saw 22\n",
      "Skipping line 1044207: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1084683: expected 15 fields, saw 22\n",
      "Skipping line 1093288: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1136430: expected 15 fields, saw 22\n",
      "Skipping line 1139815: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1179821: expected 15 fields, saw 22\n",
      "Skipping line 1195351: expected 15 fields, saw 22\n",
      "Skipping line 1202007: expected 15 fields, saw 22\n",
      "Skipping line 1224868: expected 15 fields, saw 22\n",
      "Skipping line 1232490: expected 15 fields, saw 22\n",
      "Skipping line 1238697: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1258654: expected 15 fields, saw 22\n",
      "Skipping line 1279948: expected 15 fields, saw 22\n",
      "Skipping line 1294360: expected 15 fields, saw 22\n",
      "Skipping line 1302240: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1413654: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1687095: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1805966: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1892134: expected 15 fields, saw 22\n",
      "\n",
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/1150709939.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_data = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Reading the data from the tsv (Amazon Kitchen dataset) file as a Pandas frame\n",
    "full_data = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract only Review and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0                US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
      "1                US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
      "2                US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
      "3                US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
      "4                US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
      "...             ...          ...             ...         ...             ...   \n",
      "2640249          US     53005790   RLI7EI10S7SN0  B00000DM9M       223408988   \n",
      "2640250          US     52188548  R1F3SRK9MHE6A3  B00000DM9M       223408988   \n",
      "2640251          US     52090046  R23V0C4NRJL8EM  0807865001       307284585   \n",
      "2640252          US     52503173  R13ZAE1ATEUC1T  1572313188       870359649   \n",
      "2640253          US     52585611   RE8J5O2GY04NN  1572313188       870359649   \n",
      "\n",
      "                                             product_title product_category  \\\n",
      "0           Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
      "1                Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
      "2        Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
      "3        AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
      "4        Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
      "...                                                    ...              ...   \n",
      "2640249                 PalmOne III Leather Belt Clip Case  Office Products   \n",
      "2640250                 PalmOne III Leather Belt Clip Case  Office Products   \n",
      "2640251                  Gods and Heroes of Ancient Greece  Office Products   \n",
      "2640252  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
      "2640253  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
      "\n",
      "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0                 5            0.0          0.0    N                 Y   \n",
      "1                 5            0.0          1.0    N                 Y   \n",
      "2                 5            0.0          0.0    N                 Y   \n",
      "3                 1            2.0          3.0    N                 Y   \n",
      "4                 4            0.0          0.0    N                 Y   \n",
      "...             ...            ...          ...  ...               ...   \n",
      "2640249           4           26.0         26.0    N                 N   \n",
      "2640250           4           18.0         18.0    N                 N   \n",
      "2640251           4            9.0         16.0    N                 N   \n",
      "2640252           5            0.0          0.0    N                 N   \n",
      "2640253           5            0.0          0.0    N                 N   \n",
      "\n",
      "                                           review_headline  \\\n",
      "0                                               Five Stars   \n",
      "1        Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
      "2                            but I am sure I will like it.   \n",
      "3        and the shredder was dirty and the bin was par...   \n",
      "4                                               Four Stars   \n",
      "...                                                    ...   \n",
      "2640249  Great value! A must if you hate to carry thing...   \n",
      "2640250          Attaches the Palm Pilot like an appendage   \n",
      "2640251  Excellent information, pictures and stories, I...   \n",
      "2640252                                         class text   \n",
      "2640253                                 Microsoft's Finest   \n",
      "\n",
      "                                               review_body review_date  \n",
      "0                                           Great product.  2015-08-31  \n",
      "1        What's to say about this commodity item except...  2015-08-31  \n",
      "2          Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
      "3        Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
      "4                          Gorgeous colors and easy to use  2015-08-31  \n",
      "...                                                    ...         ...  \n",
      "2640249  I can't live anymore whithout my Palm III. But...  1998-12-07  \n",
      "2640250  Although the Palm Pilot is thin and compact it...  1998-11-30  \n",
      "2640251  This book had a lot of great content without b...  1998-10-15  \n",
      "2640252  I am teaching a course in Excel and am using t...  1998-08-22  \n",
      "2640253  A very comprehensive layout of exactly how Vis...  1998-07-15  \n",
      "\n",
      "[2640254 rows x 15 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/79198575.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['star_rating'] = pd.to_numeric(full_data['star_rating'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Printing the data frame that contains the entire dataset from the tsv file\n",
    "print(full_data)\n",
    "\n",
    "# Keep only the Reviews and Ratings fields from the full data\n",
    "df = full_data[['review_body', 'star_rating']]\n",
    "\n",
    "# Converting 'star_rating' to numeric values\n",
    "df['star_rating'] = pd.to_numeric(full_data['star_rating'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a Balanced Data Set and prepare a Testing and Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset we have extracted in the previous step, creating a balanced data set for each of the rating that we have from 1 to 5. Then I had added the sentiment column based on the ratings that we have and then I have split it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/2411086817.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the Dataset has been balanced out:\n",
      "\n",
      "Star_rating  Count\n",
      "5    50000\n",
      "1    50000\n",
      "4    50000\n",
      "2    50000\n",
      "3    50000\n",
      "Name: star_rating, dtype: int64\n",
      "Checking if the Sentiments has been balanced out:\n",
      "\n",
      "Sentiment  Count\n",
      "1    100000\n",
      "2    100000\n",
      "3     50000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Printing the Test and the Training set data sizes\n",
      "Train dataset size: 200000\n",
      "Test dataset size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in 'star_rating' column\n",
    "unique_ratings = df['star_rating'].unique()\n",
    "# print(\"Unique ratings:\", unique_ratings)\n",
    "\n",
    "# Convert 'star_rating' column to integer, handling errors\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in 'star_rating' column\n",
    "df = df.dropna(subset=['star_rating'])\n",
    "\n",
    "# Convert 'star_rating' column to integer\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        return review, rating\n",
    "\n",
    "# Build balanced dataset\n",
    "ratings = df['star_rating'].unique()\n",
    "balanced_data = pd.DataFrame(columns=df.columns)\n",
    "for rating in ratings:\n",
    "    subset = df[df['star_rating'] == rating]\n",
    "    if len(subset) >= 50000:\n",
    "        subset = subset.sample(n=50000, random_state=42)\n",
    "    balanced_data = pd.concat([balanced_data, subset])\n",
    "\n",
    "# Create ternary labels\n",
    "balanced_data['sentiment'] = np.where(balanced_data['star_rating'] > 3, 1, \n",
    "                                      np.where(balanced_data['star_rating'] < 3, 2, 3))\n",
    "\n",
    "print(\"Checking if the Dataset has been balanced out:\\n\")\n",
    "print(\"Star_rating  Count\")\n",
    "print(balanced_data['star_rating'].value_counts())\n",
    "\n",
    "print(\"Checking if the Sentiments has been balanced out:\\n\")\n",
    "print(\"Sentiment  Count\")\n",
    "print(balanced_data['sentiment'].value_counts())\n",
    "\n",
    "# Perform train-test split\n",
    "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define train and test datasets\n",
    "train_dataset = AmazonReviewsDataset(train_data['review_body'].values, train_data['sentiment'].values)\n",
    "test_dataset = AmazonReviewsDataset(test_data['review_body'].values, test_data['sentiment'].values)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print sizes of train and test datasets\n",
    "print(\"\\nPrinting the Test and the Training set data sizes\")\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Training and Testing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_data['review_body'],\n",
    "                                                    balanced_data['sentiment'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data we have split.\n",
    "\n",
    "1. Removing Contractions\n",
    "2. Removing the unnecessary characters\n",
    "3. Remving any HTML links\n",
    "4. Converting to lower cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/3895173410.py:75: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/3895173410.py:75: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Printing the Average lenght of Reviews Before and After Cleaning====================\n",
      "\n",
      "Average Length of Reviews (Before Cleaning): 343 characters\n",
      "Average Length of Reviews (After Cleaning): 326 characters\n"
     ]
    }
   ],
   "source": [
    "# Define a contraction map\n",
    "CONTRACTION_MAP = {\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\"\n",
    "}\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    for contraction, expansion in CONTRACTION_MAP.items():\n",
    "        text = re.sub(contraction, expansion, text)\n",
    "    return text\n",
    "\n",
    "# Preprocess the reviews\n",
    "def preprocess_reviews(reviews):\n",
    "    # Convert to lowercase and handle NaN values\n",
    "    reviews = reviews.apply(lambda x: str(x).lower() if pd.notna(x) else '')\n",
    "    \n",
    "    # Remove HTML and URLs\n",
    "    reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "    reviews = reviews.apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "    # Remove non-alphabetical characters (excluding single quote)\n",
    "    reviews = reviews.apply(lambda x: re.sub(r'[^a-zA-Z\\s\\']', '', x))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    reviews = reviews.apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    # Perform contractions\n",
    "    reviews = reviews.apply(expand_contractions)\n",
    "\n",
    "    # Return the processed text of the review\n",
    "    return reviews\n",
    "\n",
    "# Preprocess the training set\n",
    "X_train_preprocessed = preprocess_reviews(X_train)\n",
    "\n",
    "# Print average length of reviews before and after cleaning\n",
    "avg_length_before = X_train.apply(lambda x: len(str(x))).mean()\n",
    "avg_length_after = X_train_preprocessed.apply(len).mean()\n",
    "print(\"===================Printing the Average lenght of Reviews Before and After Cleaning====================\")\n",
    "print(f\"\\nAverage Length of Reviews (Before Cleaning): {int(avg_length_before)} characters\")\n",
    "print(f\"Average Length of Reviews (After Cleaning): {int(avg_length_after)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK to remove the stop words that are unnecessary for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Printing Sample Reviews Before and After Pre-processing =============\n",
      "\n",
      "Sample Review 1958949 Before Pre-processing:\n",
      "this is a good ink pad it seems to be wearing a little sooner than i expected but it still inks everything so no complaints\n",
      "\n",
      "Sample Review 1958949 After NLTK Pre-processing:\n",
      "good ink pad seems wearing little sooner expected still ink everything complaint\n",
      "\n",
      "Sample Review 983035 Before Pre-processing:\n",
      "it is fine and will function well but i expected a little more pizazz for the price just a plain black matt\n",
      "\n",
      "Sample Review 983035 After NLTK Pre-processing:\n",
      "fine function well expected little pizazz price plain black matt\n",
      "\n",
      "Sample Review 2320561 Before Pre-processing:\n",
      "this was a pretty cheap scale i wanted to weigh envelopes and packages at work so i could just use stamps instead of waiting in line at the post office it does the job and seems to weigh accurately as i have not had anything returned without enough postage i am using it with batteries and have done so for about months without having to change them although i only use it a couple times a week so hopefully the batteries will last a good long while the power cord it comes with is kind of short it would be nice if you could program it with shipping charges so you could see the price for shipping rather than just the weight but that is probably a feature that would add more cost to it every time i turn it off it defaults to grams and i weigh in ounces so i wish it would stay on my selection but that is only a minor inconvenience overall nothing fancy but it is cheap and does the job of weighing stuff\n",
      "\n",
      "Sample Review 2320561 After NLTK Pre-processing:\n",
      "pretty cheap scale wanted weigh envelope package work could use stamp instead waiting line post office job seems weigh accurately anything returned without enough postage using battery done month without change although use couple time week hopefully battery last good long power cord come kind short would nice could program shipping charge could see price shipping rather weight probably feature would add cost every time turn default gram weigh ounce wish would stay selection minor inconvenience overall nothing fancy cheap job weighing stuff\n",
      "\n",
      "=================Printing the Average lenght of Reviews Before and After Pre-processing==================\n",
      "\n",
      "Average Length of Reviews (Before NLTK Processing): 326 characters\n",
      "Average Length of Reviews (After NLTK Processing): 201 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK's stopwords and WordNet lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to remove stop words and perform lemmatization\n",
    "def preprocess_nltk(review):\n",
    "    if pd.notna(review):\n",
    "        words = nltk.word_tokenize(str(review).lower())  # Convert to lowercase\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Preprocess the training set using NLTK\n",
    "X_train_nltk_preprocessed = X_train_preprocessed.apply(preprocess_nltk)\n",
    "\n",
    "# Print three sample reviews before and after NLTK preprocessing\n",
    "sample_reviews_indices = X_train_preprocessed.sample(3).index\n",
    "\n",
    "print(\"============ Printing Sample Reviews Before and After Pre-processing =============\")\n",
    "for index in sample_reviews_indices:\n",
    "    print(f\"\\nSample Review {index} Before Pre-processing:\")\n",
    "    print(X_train_preprocessed.loc[index])\n",
    "\n",
    "    print(f\"\\nSample Review {index} After NLTK Pre-processing:\")\n",
    "    print(X_train_nltk_preprocessed.loc[index])\n",
    "\n",
    "# Print average length of reviews before and after NLTK processing\n",
    "avg_length_before_nltk = X_train_preprocessed.apply(len).mean()\n",
    "avg_length_after_nltk = X_train_nltk_preprocessed.apply(len).mean()\n",
    "print(\"\\n=================Printing the Average lenght of Reviews Before and After Pre-processing==================\")\n",
    "print(f\"\\nAverage Length of Reviews (Before NLTK Processing): {int(avg_length_before_nltk)} characters\")\n",
    "print(f\"Average Length of Reviews (After NLTK Processing): {int(avg_length_after_nltk)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  2. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Score using the Word2Vec Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have to use binary data for Simple Models(Perceptron and SVM), so we need to remove the neutral reviews with class 3 and keep only Class 1 and Class 2.\n",
    "\n",
    "For Simple models classification, we need to change the class labels from 1 and 2 to 0 and 1, as the class labels should always start from '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining features and targets for training dataset\n",
    "train_data = pd.concat([X_train_nltk_preprocessed, y_train], axis=1)\n",
    "train_data_filtered = train_data[train_data['sentiment'] != 3]\n",
    "X_train_binary = train_data_filtered['review_body']\n",
    "y_train_binary = train_data_filtered['sentiment']\n",
    "\n",
    "# Joining features and targets for testing dataset\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data_filtered = test_data[test_data['sentiment'] != 3]\n",
    "X_test_binary = test_data_filtered['review_body']\n",
    "y_test_binary = test_data_filtered['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Pre-trained Model from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = [('queen', 0.7118192911148071)]\n",
      "Excellent ~ Outstanding = [('oustanding', 0.750198483467102)]\n",
      "Marriage - Man + Woman = [('marriages', 0.7118672728538513)]\n",
      "Good ~ Bad = [('excellent', 0.46134573221206665)]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract Word2Vec features for a given sentence\n",
    "def extract_word2vec_features(sentence, model, vector_size):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.key_to_index:\n",
    "            word_vectors.append(model.get_vector(word))\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no word vectors found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)  # Return average word vector\n",
    "\n",
    "# Examples to check semantic similarities\n",
    "example1 = wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", example1)\n",
    "\n",
    "example2 = wv.most_similar(positive=['excellent', 'outstanding'], topn=1)\n",
    "print(\"Excellent ~ Outstanding =\", example2)\n",
    "\n",
    "example3 = wv.most_similar(positive=['marriage', 'woman'], negative=['man'], topn=1)\n",
    "print(\"Marriage - Man + Woman =\", example3)\n",
    "\n",
    "example4 = wv.most_similar(positive=['good'], negative=['bad'], topn=1)\n",
    "print(\"Good ~ Bad =\", example4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Score using the Word2Vec model with the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200000it [00:21, 9324.76it/s] \n"
     ]
    }
   ],
   "source": [
    "X_train_nltk_preprocessed_new = []\n",
    "\n",
    "for e, k in tqdm(enumerate(X_train_nltk_preprocessed.to_list())):\n",
    "    try:\n",
    "        X_train_nltk_preprocessed_new.append(word_tokenize(k))\n",
    "    except:\n",
    "        pass    \n",
    "def get_doc_embedding(doc):\n",
    "    words = doc.lower().split()\n",
    "    return wv.get_mean_vector(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing the Pre-trained model and making it custom trained with the data we have from the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train the Word2Vec model\n",
    "model_own = Word2Vec(X_train_nltk_preprocessed_new, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman (Own Model) = [('romney', 0.5904972553253174)]\n",
      "Excellent ~ Outstanding (Own Model) = [('superb', 0.8473477363586426)]\n",
      "Marriage - Man + Woman (Own Model) = [('romney', 0.5809527039527893)]\n",
      "Good ~ Bad (Own Model) = [('excellent', 0.49807488918304443)]\n",
      "\n",
      "Semantic similarity (Pretrained Model): [('queen', 0.7118192911148071)]\n",
      "Semantic similarity (Pretrained Model): [('oustanding', 0.750198483467102)]\n",
      "Semantic similarity (Pretrained Model): [('marriages', 0.7118672728538513)]\n",
      "Semantic similarity (Pretrained Model): [('excellent', 0.46134573221206665)]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract Word2Vec features for a given sentence using the trained model\n",
    "def extract_word2vec_features_own(sentence, model, vector_size):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv.key_to_index:\n",
    "            word_vectors.append(model.wv.get_vector(word))\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no word vectors found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)  # Return average word vector\n",
    "\n",
    "# Examples to check semantic similarities using your own model\n",
    "if 'king' in model_own.wv.key_to_index and 'woman' in model_own.wv.key_to_index:\n",
    "    example1_own = model_own.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "    print(\"King - Man + Woman (Own Model) =\", example1_own)\n",
    "else:\n",
    "    print(\"'good' is not present in the vocabulary.\")\n",
    "\n",
    "if 'excellent' in model_own.wv.key_to_index and 'outstanding' in model_own.wv.key_to_index:\n",
    "    example2_own = model_own.wv.most_similar(positive=['excellent', 'outstanding'], topn=1)\n",
    "    print(\"Excellent ~ Outstanding (Own Model) =\", example2_own)\n",
    "else:\n",
    "    print(\"'excellent' and/or 'outstanding' are not present in the vocabulary.\")\n",
    "    \n",
    "if 'marriage' in model_own.wv.key_to_index and 'woman' in model_own.wv.key_to_index:\n",
    "    example3_own = model_own.wv.most_similar(positive=['marriage', 'woman'], negative=['man'], topn=1)\n",
    "    print(\"Marriage - Man + Woman (Own Model) =\", example3_own)\n",
    "else:\n",
    "    print(\"'marriage' and/or 'woman' are not present in the vocabulary.\")\n",
    "    \n",
    "if 'good' in model_own.wv.key_to_index:\n",
    "    example4_own = model_own.wv.most_similar(positive=['good'], negative=['bad'], topn=1)\n",
    "    print(\"Good ~ Bad (Own Model) =\", example4_own)\n",
    "else:\n",
    "    print(\"'marriage' and/or 'woman' are not present in the vocabulary.\")\n",
    "\n",
    "# Compare semantic similarities between pretrained and own models\n",
    "print(\"\\nSemantic similarity (Pretrained Model):\", example1)\n",
    "print(\"Semantic similarity (Pretrained Model):\", example2)\n",
    "print(\"Semantic similarity (Pretrained Model):\", example3)\n",
    "print(\"Semantic similarity (Pretrained Model):\", example4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing vectors generated by pre-trained Word2Vec models and those trained on specific datasets reveals nuanced trade-offs. Pre-trained models, leveraging vast corpora, excel in capturing broad semantic similarities but may lack fine-grained domain specificity. Conversely, self-generated vectors, tailored to specific contexts, offer potential for domain-specific insights but require representative data and entail computational costs. Evaluating both models on relevant tasks like word similarity or downstream applications is crucial to determining which better encodes semantic relationships for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the TF-IDF Features from Dataset and train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the TF-IDF feature vectors from the initial dataset, and using that data set to train the Perceptron and SVM, and calculate the accuracy on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train_tfidf: (160200, 112187)\n",
      "Shape of X_test_tfidf: (39800, 112187)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000000)\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_binary)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_binary.apply(preprocess_nltk))\n",
    "\n",
    "# Print the shape of the TF-IDF matrices\n",
    "print(f\"\\nShape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Single Layer Perceptron using TF-IDF Features: 0.8129396984924623\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=1000)\n",
    "\n",
    "perceptron.fit(X_train_tfidf, y_train_binary)\n",
    "y_pred = perceptron.predict(X_test_tfidf)\n",
    "accuracy_perceptron_tf_idf = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the Single Layer Perceptron using TF-IDF Features: {accuracy_perceptron_tf_idf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM using TF-IDF Features: 0.8603015075376884\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_train_tfidf, y_train_binary)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "accuracy_svm_tf_idf = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the SVM using TF-IDF Features: {accuracy_svm_tf_idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Pre-trained Features from Dataset and train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Word2Vec feature vectors on the Pre-trained model from the initial dataset, and using that data set to train the Perceptron and SVM, and calculate the accuracy on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 160200/160200 [01:30<00:00, 1776.51it/s]\n",
      "100%|███████████████████████████████████| 39800/39800 [00:34<00:00, 1159.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160200, 300), (39800, 300))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v_binary = []\n",
    "X_test_w2v_binary = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_binary):\n",
    "    X_train_w2v_binary.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test_binary):\n",
    "    X_test_w2v_binary.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v_binary = np.array(X_train_w2v_binary)\n",
    "X_test_w2v_binary = np.array(X_test_w2v_binary)\n",
    "\n",
    "X_train_w2v_binary.shape, X_test_w2v_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_one_hot = to_categorical(y_train_binary)\n",
    "# y_test_one_hot = to_categorical(y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Single Layer Perceptron using Word2Vec Pretrained Features: 0.4999497487437186\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=1000)\n",
    "\n",
    "perceptron.fit(X_train_w2v_binary, y_train_binary)\n",
    "y_pred = perceptron.predict(X_test_w2v_binary)\n",
    "accuracy_perceptron_w2v_pre = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the Single Layer Perceptron using Word2Vec Pretrained Features: {accuracy_perceptron_w2v_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM using Word2Vec Pretrained Features: 0.6078391959798995\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_train_w2v_binary, y_train_binary)\n",
    "y_pred = svm.predict(X_test_w2v_binary)\n",
    "accuracy_svm_w2v_pre = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the SVM using Word2Vec Pretrained Features: {accuracy_svm_w2v_pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Custom Features from Dataset and train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Word2Vec feature vectors on the Custom trained model from the initial dataset, and using that data set to train the Perceptron and SVM, and calculate the accuracy on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 160200/160200 [01:28<00:00, 1804.61it/s]\n",
      "100%|███████████████████████████████████| 39800/39800 [00:30<00:00, 1300.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160200, 300), (39800, 300))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return model_own.wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v_own_binary = []\n",
    "X_test_w2v_own_binary = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_binary):\n",
    "    X_train_w2v_own_binary.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test_binary):\n",
    "    X_test_w2v_own_binary.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v_own_binary = np.array(X_train_w2v_own_binary)\n",
    "X_test_w2v_own_binary = np.array(X_test_w2v_own_binary)\n",
    "\n",
    "X_train_w2v_own_binary.shape, X_test_w2v_own_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Single Layer Perceptron using Word2Vec Custom Features: 0.5012311557788944\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=1000)\n",
    "\n",
    "perceptron.fit(X_train_w2v_own_binary, y_train_binary)\n",
    "y_pred = perceptron.predict(X_test_w2v_own_binary)\n",
    "accuracy_perceptron_w2v_own = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the Single Layer Perceptron using Word2Vec Custom Features: {accuracy_perceptron_w2v_own}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM using Word2Vec Custom Features: 0.6076130653266332\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_train_w2v_own_binary, y_train_binary)\n",
    "y_pred = svm.predict(X_test_w2v_own_binary)\n",
    "accuracy_svm_w2v_own = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the SVM using Word2Vec Custom Features: {accuracy_svm_w2v_own}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same binary dataset vectors that we have generated from the Word2Vec Pre-trained and Custom models that we have done in Question 3 to train the Multi-layer perceptron and evaluate the accuracy on the testing dataset.\n",
    "\n",
    "Here as well we need to encode the classes to 0 and 1, as the class labels shall start from '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "y_train_np = np.array(y_train_binary)\n",
    "y_test_np = np.array(y_test_binary)\n",
    "\n",
    "# Convert labels to binary format (0 for class 1, 1 for class 2)\n",
    "y_train_binary = np.where(y_train_np == 1, 0, 1)\n",
    "y_test_binary = np.where(y_test_np == 1, 0, 1)\n",
    "\n",
    "# Define the MLP Model for binary classification\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(BinaryMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Training Loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Custom Features and train a Multi Layer Perceptron with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3323.425560414791\n",
      "Epoch 2/50, Loss: 3242.8365510106087\n",
      "Epoch 3/50, Loss: 3224.7675822377205\n",
      "Epoch 4/50, Loss: 3214.373399734497\n",
      "Epoch 5/50, Loss: 3209.4932764172554\n",
      "Epoch 6/50, Loss: 3200.5022310614586\n",
      "Epoch 7/50, Loss: 3193.3247091174126\n",
      "Epoch 8/50, Loss: 3185.9956729114056\n",
      "Epoch 9/50, Loss: 3181.4531664848328\n",
      "Epoch 10/50, Loss: 3176.8937999606133\n",
      "Epoch 11/50, Loss: 3175.9828359484673\n",
      "Epoch 12/50, Loss: 3170.8038108944893\n",
      "Epoch 13/50, Loss: 3168.285962700844\n",
      "Epoch 14/50, Loss: 3164.8113036751747\n",
      "Epoch 15/50, Loss: 3161.546014279127\n",
      "Epoch 16/50, Loss: 3160.4918980002403\n",
      "Epoch 17/50, Loss: 3157.501613199711\n",
      "Epoch 18/50, Loss: 3157.6596927046776\n",
      "Epoch 19/50, Loss: 3155.937397301197\n",
      "Epoch 20/50, Loss: 3153.528224468231\n",
      "Epoch 21/50, Loss: 3151.404807239771\n",
      "Epoch 22/50, Loss: 3151.1297699809074\n",
      "Epoch 23/50, Loss: 3149.1825039088726\n",
      "Epoch 24/50, Loss: 3149.206249654293\n",
      "Epoch 25/50, Loss: 3146.9454906880856\n",
      "Epoch 26/50, Loss: 3145.9316977858543\n",
      "Epoch 27/50, Loss: 3147.08208245039\n",
      "Epoch 28/50, Loss: 3144.826644539833\n",
      "Epoch 29/50, Loss: 3143.413956552744\n",
      "Epoch 30/50, Loss: 3145.089313417673\n",
      "Epoch 31/50, Loss: 3143.160977780819\n",
      "Epoch 32/50, Loss: 3143.921137303114\n",
      "Epoch 33/50, Loss: 3140.6391310095787\n",
      "Epoch 34/50, Loss: 3141.163193643093\n",
      "Epoch 35/50, Loss: 3139.8949775993824\n",
      "Epoch 36/50, Loss: 3138.606913626194\n",
      "Epoch 37/50, Loss: 3139.145885795355\n",
      "Epoch 38/50, Loss: 3137.8794299662113\n",
      "Epoch 39/50, Loss: 3138.6251110732555\n",
      "Epoch 40/50, Loss: 3137.357636541128\n",
      "Epoch 41/50, Loss: 3136.401563555002\n",
      "Epoch 42/50, Loss: 3136.378142297268\n",
      "Epoch 43/50, Loss: 3135.6460728645325\n",
      "Epoch 44/50, Loss: 3134.1910824477673\n",
      "Epoch 45/50, Loss: 3135.3988238573074\n",
      "Epoch 46/50, Loss: 3134.54019472003\n",
      "Epoch 47/50, Loss: 3133.9537192881107\n",
      "Epoch 48/50, Loss: 3133.610229164362\n",
      "Epoch 49/50, Loss: 3132.686781704426\n",
      "Epoch 50/50, Loss: 3133.3254142701626\n",
      "Accuracy: 0.6349748743718593\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_own_binary = TensorDataset(torch.tensor(X_train_w2v_own_binary, dtype=torch.float32), torch.tensor(y_train_binary, dtype=torch.long))\n",
    "test_dataset_own_binary = TensorDataset(torch.tensor(X_test_w2v_own_binary, dtype=torch.float32), torch.tensor(y_test_binary, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_own_binary = DataLoader(train_dataset_own_binary, batch_size=32, shuffle=True)\n",
    "test_loader_own_binary = DataLoader(test_dataset_own_binary, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v_own_binary.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Binary classification (classes 1 and 2)\n",
    "binary_model_own_binary = BinaryMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(binary_model_own_binary.parameters(), lr=0.001)\n",
    "\n",
    "# Train the binary classification model\n",
    "num_epochs = 50\n",
    "train(binary_model_own_binary, train_loader_own_binary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the binary classification model\n",
    "evaluate(binary_model_own_binary, test_loader_own_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Pretrained Features and train a Multi Layer Perceptron with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3344.9393923282623\n",
      "Epoch 2/50, Loss: 3267.1416442990303\n",
      "Epoch 3/50, Loss: 3232.56443169713\n",
      "Epoch 4/50, Loss: 3216.721622556448\n",
      "Epoch 5/50, Loss: 3211.3986123502254\n",
      "Epoch 6/50, Loss: 3207.7164747714996\n",
      "Epoch 7/50, Loss: 3202.744212627411\n",
      "Epoch 8/50, Loss: 3200.1807994246483\n",
      "Epoch 9/50, Loss: 3194.121215969324\n",
      "Epoch 10/50, Loss: 3189.0789400935173\n",
      "Epoch 11/50, Loss: 3185.3685515522957\n",
      "Epoch 12/50, Loss: 3179.5080044567585\n",
      "Epoch 13/50, Loss: 3172.4399179518223\n",
      "Epoch 14/50, Loss: 3169.3857010900974\n",
      "Epoch 15/50, Loss: 3167.2382534742355\n",
      "Epoch 16/50, Loss: 3163.475892752409\n",
      "Epoch 17/50, Loss: 3164.04864063859\n",
      "Epoch 18/50, Loss: 3159.970018863678\n",
      "Epoch 19/50, Loss: 3158.055665344\n",
      "Epoch 20/50, Loss: 3156.751287251711\n",
      "Epoch 21/50, Loss: 3156.282939761877\n",
      "Epoch 22/50, Loss: 3154.486574202776\n",
      "Epoch 23/50, Loss: 3153.950118690729\n",
      "Epoch 24/50, Loss: 3152.3372714221478\n",
      "Epoch 25/50, Loss: 3151.794505864382\n",
      "Epoch 26/50, Loss: 3150.168945878744\n",
      "Epoch 27/50, Loss: 3149.924498349428\n",
      "Epoch 28/50, Loss: 3148.044831752777\n",
      "Epoch 29/50, Loss: 3148.335063278675\n",
      "Epoch 30/50, Loss: 3147.457699537277\n",
      "Epoch 31/50, Loss: 3145.4660854637623\n",
      "Epoch 32/50, Loss: 3145.5864459574223\n",
      "Epoch 33/50, Loss: 3144.6990844011307\n",
      "Epoch 34/50, Loss: 3144.0857751965523\n",
      "Epoch 35/50, Loss: 3144.998258292675\n",
      "Epoch 36/50, Loss: 3143.953347861767\n",
      "Epoch 37/50, Loss: 3143.556082755327\n",
      "Epoch 38/50, Loss: 3143.4792056381702\n",
      "Epoch 39/50, Loss: 3141.8969056606293\n",
      "Epoch 40/50, Loss: 3141.6346136033535\n",
      "Epoch 41/50, Loss: 3140.0632943212986\n",
      "Epoch 42/50, Loss: 3140.9293306469917\n",
      "Epoch 43/50, Loss: 3140.620075672865\n",
      "Epoch 44/50, Loss: 3140.5128760635853\n",
      "Epoch 45/50, Loss: 3137.908288091421\n",
      "Epoch 46/50, Loss: 3138.1942223012447\n",
      "Epoch 47/50, Loss: 3137.895871460438\n",
      "Epoch 48/50, Loss: 3139.127146035433\n",
      "Epoch 49/50, Loss: 3138.0721495449543\n",
      "Epoch 50/50, Loss: 3136.5219447016716\n",
      "Accuracy: 0.6264321608040201\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_pre_binary = TensorDataset(torch.tensor(X_train_w2v_binary, dtype=torch.float32), torch.tensor(y_train_binary, dtype=torch.long))\n",
    "test_dataset_pre_binary = TensorDataset(torch.tensor(X_test_w2v_binary, dtype=torch.float32), torch.tensor(y_test_binary, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_pre_binary = DataLoader(train_dataset_pre_binary, batch_size=32, shuffle=True)\n",
    "test_loader_pre_binary = DataLoader(test_dataset_pre_binary, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v_binary.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Binary classification (classes 1 and 2)\n",
    "binary_model_pre_binary = BinaryMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(binary_model_pre_binary.parameters(), lr=0.001)\n",
    "\n",
    "# Train the binary classification model\n",
    "num_epochs = 50\n",
    "train(binary_model_pre_binary, train_loader_pre_binary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the binary classification model\n",
    "evaluate(binary_model_pre_binary, test_loader_pre_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Ternary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we shall be using the original dataset, that has 3 classes 1, 2 and 3. then calculating the Word2Vec vectors for both the pre-trained and Custom model and using them to train the multi-layer perceptron and evaluating on the testing set.\n",
    "\n",
    "Here in this case as well we need to encode the class labels from 1 to 3 to 0 to 2, as the class labels shall start from '0' only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200000/200000 [01:46<00:00, 1873.31it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:39<00:00, 1254.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return model_own.wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v_own = []\n",
    "X_test_w2v_own = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_nltk_preprocessed):\n",
    "    X_train_w2v_own.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test):\n",
    "    X_test_w2v_own.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v_own = np.array(X_train_w2v_own)\n",
    "X_test_w2v_own = np.array(X_test_w2v_own)\n",
    "\n",
    "X_train_w2v_own.shape, X_test_w2v_own.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200000/200000 [01:53<00:00, 1759.83it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:43<00:00, 1142.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v = []\n",
    "X_test_w2v = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_nltk_preprocessed):\n",
    "    X_train_w2v.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test):\n",
    "    X_test_w2v.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v = np.array(X_train_w2v)\n",
    "X_test_w2v = np.array(X_test_w2v)\n",
    "\n",
    "X_train_w2v.shape, X_test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Convert labels to integers ranging from 0 to 2\n",
    "y_train_encoded = [label - 1 for label in y_train_np]  # Assuming labels start from 1\n",
    "y_test_encoded = [label - 1 for label in y_test_np]  # Assuming labels start from 1\n",
    "\n",
    "# Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Training Loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Pre-trained Features and train a Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6505.194889605045\n",
      "Epoch 2/50, Loss: 6407.516536474228\n",
      "Epoch 3/50, Loss: 6372.225348472595\n",
      "Epoch 4/50, Loss: 6355.613764286041\n",
      "Epoch 5/50, Loss: 6347.46262139082\n",
      "Epoch 6/50, Loss: 6340.601443588734\n",
      "Epoch 7/50, Loss: 6331.0599210858345\n",
      "Epoch 8/50, Loss: 6315.430656015873\n",
      "Epoch 9/50, Loss: 6306.298391401768\n",
      "Epoch 10/50, Loss: 6300.394243955612\n",
      "Epoch 11/50, Loss: 6293.240439116955\n",
      "Epoch 12/50, Loss: 6289.825047254562\n",
      "Epoch 13/50, Loss: 6285.750542700291\n",
      "Epoch 14/50, Loss: 6281.849463224411\n",
      "Epoch 15/50, Loss: 6279.120568394661\n",
      "Epoch 16/50, Loss: 6277.236977458\n",
      "Epoch 17/50, Loss: 6274.526634931564\n",
      "Epoch 18/50, Loss: 6273.958921611309\n",
      "Epoch 19/50, Loss: 6268.318421125412\n",
      "Epoch 20/50, Loss: 6268.669386148453\n",
      "Epoch 21/50, Loss: 6265.7270964980125\n",
      "Epoch 22/50, Loss: 6263.161739349365\n",
      "Epoch 23/50, Loss: 6262.33771365881\n",
      "Epoch 24/50, Loss: 6261.571010649204\n",
      "Epoch 25/50, Loss: 6260.096761286259\n",
      "Epoch 26/50, Loss: 6260.204388618469\n",
      "Epoch 27/50, Loss: 6258.781262874603\n",
      "Epoch 28/50, Loss: 6256.204732000828\n",
      "Epoch 29/50, Loss: 6254.868431091309\n",
      "Epoch 30/50, Loss: 6253.441541552544\n",
      "Epoch 31/50, Loss: 6251.667254507542\n",
      "Epoch 32/50, Loss: 6251.896208524704\n",
      "Epoch 33/50, Loss: 6249.928118169308\n",
      "Epoch 34/50, Loss: 6250.055064558983\n",
      "Epoch 35/50, Loss: 6247.988093972206\n",
      "Epoch 36/50, Loss: 6246.2828566432\n",
      "Epoch 37/50, Loss: 6246.4094088077545\n",
      "Epoch 38/50, Loss: 6245.59342867136\n",
      "Epoch 39/50, Loss: 6243.528662264347\n",
      "Epoch 40/50, Loss: 6243.890145599842\n",
      "Epoch 41/50, Loss: 6244.921311080456\n",
      "Epoch 42/50, Loss: 6241.981376111507\n",
      "Epoch 43/50, Loss: 6242.319493114948\n",
      "Epoch 44/50, Loss: 6241.5356143713\n",
      "Epoch 45/50, Loss: 6239.235330283642\n",
      "Epoch 46/50, Loss: 6240.063661873341\n",
      "Epoch 47/50, Loss: 6238.990886032581\n",
      "Epoch 48/50, Loss: 6238.533412337303\n",
      "Epoch 49/50, Loss: 6237.538753211498\n",
      "Epoch 50/50, Loss: 6237.690585613251\n",
      "Accuracy: 0.5133\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_pre = TensorDataset(torch.tensor(X_train_w2v, dtype=torch.float32), torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_dataset_pre = TensorDataset(torch.tensor(X_test_w2v, dtype=torch.float32), torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_pre = DataLoader(train_dataset_pre, batch_size=32, shuffle=True)\n",
    "test_loader_pre = DataLoader(test_dataset_pre, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_pre = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_pre.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_pre, train_loader_pre, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_pre, test_loader_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Custom Features and train a Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6498.780159831047\n",
      "Epoch 2/50, Loss: 6400.97633677721\n",
      "Epoch 3/50, Loss: 6376.639673233032\n",
      "Epoch 4/50, Loss: 6358.732979893684\n",
      "Epoch 5/50, Loss: 6347.87987112999\n",
      "Epoch 6/50, Loss: 6341.1168175935745\n",
      "Epoch 7/50, Loss: 6332.512089669704\n",
      "Epoch 8/50, Loss: 6328.183353543282\n",
      "Epoch 9/50, Loss: 6323.460064649582\n",
      "Epoch 10/50, Loss: 6319.396956145763\n",
      "Epoch 11/50, Loss: 6314.587701499462\n",
      "Epoch 12/50, Loss: 6310.881038248539\n",
      "Epoch 13/50, Loss: 6307.944660484791\n",
      "Epoch 14/50, Loss: 6303.935755848885\n",
      "Epoch 15/50, Loss: 6302.715979993343\n",
      "Epoch 16/50, Loss: 6298.875631213188\n",
      "Epoch 17/50, Loss: 6296.430803596973\n",
      "Epoch 18/50, Loss: 6293.912975907326\n",
      "Epoch 19/50, Loss: 6291.618820667267\n",
      "Epoch 20/50, Loss: 6288.986372232437\n",
      "Epoch 21/50, Loss: 6285.127147078514\n",
      "Epoch 22/50, Loss: 6283.881210565567\n",
      "Epoch 23/50, Loss: 6279.84032189846\n",
      "Epoch 24/50, Loss: 6279.9689974188805\n",
      "Epoch 25/50, Loss: 6277.729133725166\n",
      "Epoch 26/50, Loss: 6276.328289449215\n",
      "Epoch 27/50, Loss: 6273.352016687393\n",
      "Epoch 28/50, Loss: 6270.606888830662\n",
      "Epoch 29/50, Loss: 6268.878164708614\n",
      "Epoch 30/50, Loss: 6268.279967546463\n",
      "Epoch 31/50, Loss: 6266.174606144428\n",
      "Epoch 32/50, Loss: 6265.104053735733\n",
      "Epoch 33/50, Loss: 6264.6649406552315\n",
      "Epoch 34/50, Loss: 6262.693824112415\n",
      "Epoch 35/50, Loss: 6261.148292958736\n",
      "Epoch 36/50, Loss: 6261.369768679142\n",
      "Epoch 37/50, Loss: 6258.9298285245895\n",
      "Epoch 38/50, Loss: 6256.8996948599815\n",
      "Epoch 39/50, Loss: 6257.075434565544\n",
      "Epoch 40/50, Loss: 6255.959533751011\n",
      "Epoch 41/50, Loss: 6255.498541593552\n",
      "Epoch 42/50, Loss: 6254.348329305649\n",
      "Epoch 43/50, Loss: 6253.644511044025\n",
      "Epoch 44/50, Loss: 6252.764105439186\n",
      "Epoch 45/50, Loss: 6251.325192272663\n",
      "Epoch 46/50, Loss: 6250.597810804844\n",
      "Epoch 47/50, Loss: 6250.223139643669\n",
      "Epoch 48/50, Loss: 6249.160574257374\n",
      "Epoch 49/50, Loss: 6248.502880871296\n",
      "Epoch 50/50, Loss: 6247.869339823723\n",
      "Accuracy: 0.50938\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_own = TensorDataset(torch.tensor(X_train_w2v_own, dtype=torch.float32), torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_dataset_own = TensorDataset(torch.tensor(X_test_w2v_own, dtype=torch.float32), torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_own = DataLoader(train_dataset_own, batch_size=32, shuffle=True)\n",
    "test_loader_own = DataLoader(test_dataset_own, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v_own.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_own = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_own.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_own, train_loader_own, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_own, test_loader_own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am extracting the first 10 Word2Vec features, and using them for the ternary classification as well as using the binary features that we have extracted to train the binary Multi layer perceptron on the training dataset and evaluating their performance on the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Ternary Classification for the Word2Vec Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_31112/1352761604.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  X_train_concat_tensor_own = torch.tensor(X_train_concat_own, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6481.61340034008\n",
      "Epoch 2/50, Loss: 6385.503250360489\n",
      "Epoch 3/50, Loss: 6360.853358089924\n",
      "Epoch 4/50, Loss: 6345.465584874153\n",
      "Epoch 5/50, Loss: 6333.520486533642\n",
      "Epoch 6/50, Loss: 6324.639396846294\n",
      "Epoch 7/50, Loss: 6317.478264570236\n",
      "Epoch 8/50, Loss: 6312.688789844513\n",
      "Epoch 9/50, Loss: 6306.100179135799\n",
      "Epoch 10/50, Loss: 6300.011463165283\n",
      "Epoch 11/50, Loss: 6294.431249856949\n",
      "Epoch 12/50, Loss: 6293.446542024612\n",
      "Epoch 13/50, Loss: 6289.821278214455\n",
      "Epoch 14/50, Loss: 6288.257537126541\n",
      "Epoch 15/50, Loss: 6284.445706427097\n",
      "Epoch 16/50, Loss: 6282.852754354477\n",
      "Epoch 17/50, Loss: 6279.708474516869\n",
      "Epoch 18/50, Loss: 6279.602629005909\n",
      "Epoch 19/50, Loss: 6276.000140547752\n",
      "Epoch 20/50, Loss: 6273.767419040203\n",
      "Epoch 21/50, Loss: 6269.479419648647\n",
      "Epoch 22/50, Loss: 6268.02747040987\n",
      "Epoch 23/50, Loss: 6267.527205705643\n",
      "Epoch 24/50, Loss: 6262.9584149718285\n",
      "Epoch 25/50, Loss: 6262.649322628975\n",
      "Epoch 26/50, Loss: 6258.973907291889\n",
      "Epoch 27/50, Loss: 6258.186829328537\n",
      "Epoch 28/50, Loss: 6256.603058755398\n",
      "Epoch 29/50, Loss: 6256.090897679329\n",
      "Epoch 30/50, Loss: 6253.6809985637665\n",
      "Epoch 31/50, Loss: 6254.460513472557\n",
      "Epoch 32/50, Loss: 6252.560277581215\n",
      "Epoch 33/50, Loss: 6249.541831314564\n",
      "Epoch 34/50, Loss: 6250.814956605434\n",
      "Epoch 35/50, Loss: 6248.070578336716\n",
      "Epoch 36/50, Loss: 6246.799599528313\n",
      "Epoch 37/50, Loss: 6246.783444583416\n",
      "Epoch 38/50, Loss: 6245.462326824665\n",
      "Epoch 39/50, Loss: 6244.607045471668\n",
      "Epoch 40/50, Loss: 6242.118771910667\n",
      "Epoch 41/50, Loss: 6241.047801613808\n",
      "Epoch 42/50, Loss: 6239.572349309921\n",
      "Epoch 43/50, Loss: 6240.246558308601\n",
      "Epoch 44/50, Loss: 6238.977270483971\n",
      "Epoch 45/50, Loss: 6237.48008698225\n",
      "Epoch 46/50, Loss: 6236.673461198807\n",
      "Epoch 47/50, Loss: 6237.036950469017\n",
      "Epoch 48/50, Loss: 6234.947586297989\n",
      "Epoch 49/50, Loss: 6235.281324267387\n",
      "Epoch 50/50, Loss: 6233.786520183086\n",
      "Accuracy: 0.50792\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat_own = []\n",
    "X_test_concat_own = []\n",
    "\n",
    "for review in X_train_w2v_own:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat_own.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v_own:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat_own.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor_own = torch.tensor(X_train_concat_own, dtype=torch.float32)\n",
    "X_test_concat_tensor_own = torch.tensor(X_test_concat_own, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor_own = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor_own = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat_own = TensorDataset(X_train_concat_tensor_own, y_train_tensor_own)\n",
    "test_dataset_concat_own = TensorDataset(X_test_concat_tensor_own, y_test_tensor_own)\n",
    "\n",
    "train_loader_concat_own = DataLoader(train_dataset_concat_own, batch_size=32, shuffle=True)\n",
    "test_loader_concat_own = DataLoader(test_dataset_concat_own, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor_own.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_concat_own = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat_own.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat_own, train_loader_concat_own, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat_own, test_loader_concat_own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Ternary Classification for the Word2Vec Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6501.003310024738\n",
      "Epoch 2/50, Loss: 6411.406356275082\n",
      "Epoch 3/50, Loss: 6379.237156510353\n",
      "Epoch 4/50, Loss: 6357.855707705021\n",
      "Epoch 5/50, Loss: 6346.558179080486\n",
      "Epoch 6/50, Loss: 6336.867486059666\n",
      "Epoch 7/50, Loss: 6328.332867145538\n",
      "Epoch 8/50, Loss: 6320.53949290514\n",
      "Epoch 9/50, Loss: 6312.367366075516\n",
      "Epoch 10/50, Loss: 6304.602900922298\n",
      "Epoch 11/50, Loss: 6297.362203478813\n",
      "Epoch 12/50, Loss: 6291.952714383602\n",
      "Epoch 13/50, Loss: 6288.066511452198\n",
      "Epoch 14/50, Loss: 6285.03642898798\n",
      "Epoch 15/50, Loss: 6282.7331283688545\n",
      "Epoch 16/50, Loss: 6283.071981787682\n",
      "Epoch 17/50, Loss: 6278.795620620251\n",
      "Epoch 18/50, Loss: 6276.931710362434\n",
      "Epoch 19/50, Loss: 6272.630007445812\n",
      "Epoch 20/50, Loss: 6270.558665752411\n",
      "Epoch 21/50, Loss: 6268.948526859283\n",
      "Epoch 22/50, Loss: 6267.434275388718\n",
      "Epoch 23/50, Loss: 6267.472657203674\n",
      "Epoch 24/50, Loss: 6263.57393437624\n",
      "Epoch 25/50, Loss: 6261.93185031414\n",
      "Epoch 26/50, Loss: 6261.628469407558\n",
      "Epoch 27/50, Loss: 6259.062380969524\n",
      "Epoch 28/50, Loss: 6257.009374797344\n",
      "Epoch 29/50, Loss: 6258.113060712814\n",
      "Epoch 30/50, Loss: 6257.009604692459\n",
      "Epoch 31/50, Loss: 6256.341547608376\n",
      "Epoch 32/50, Loss: 6253.2241161465645\n",
      "Epoch 33/50, Loss: 6252.160817086697\n",
      "Epoch 34/50, Loss: 6251.296446084976\n",
      "Epoch 35/50, Loss: 6249.434270620346\n",
      "Epoch 36/50, Loss: 6248.6408215761185\n",
      "Epoch 37/50, Loss: 6247.61165034771\n",
      "Epoch 38/50, Loss: 6245.353277087212\n",
      "Epoch 39/50, Loss: 6245.599706590176\n",
      "Epoch 40/50, Loss: 6244.8497838974\n",
      "Epoch 41/50, Loss: 6243.380714297295\n",
      "Epoch 42/50, Loss: 6241.21665763855\n",
      "Epoch 43/50, Loss: 6241.91231995821\n",
      "Epoch 44/50, Loss: 6239.715795278549\n",
      "Epoch 45/50, Loss: 6238.923026382923\n",
      "Epoch 46/50, Loss: 6239.439045011997\n",
      "Epoch 47/50, Loss: 6237.387090682983\n",
      "Epoch 48/50, Loss: 6237.779659986496\n",
      "Epoch 49/50, Loss: 6237.464747846127\n",
      "Epoch 50/50, Loss: 6234.900174915791\n",
      "Accuracy: 0.50782\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat = []\n",
    "X_test_concat = []\n",
    "\n",
    "for review in X_train_w2v:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor = torch.tensor(X_train_concat, dtype=torch.float32)\n",
    "X_test_concat_tensor = torch.tensor(X_test_concat, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat = TensorDataset(X_train_concat_tensor, y_train_tensor)\n",
    "test_dataset_concat = TensorDataset(X_test_concat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_concat, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_concat, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_concat = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat, test_loader_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Binary Classification for the Word2Vec Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3356.1845531463623\n",
      "Epoch 2/50, Loss: 3271.8983495235443\n",
      "Epoch 3/50, Loss: 3233.304957896471\n",
      "Epoch 4/50, Loss: 3217.90689894557\n",
      "Epoch 5/50, Loss: 3210.572159022093\n",
      "Epoch 6/50, Loss: 3204.472539514303\n",
      "Epoch 7/50, Loss: 3197.940410375595\n",
      "Epoch 8/50, Loss: 3195.5977049171925\n",
      "Epoch 9/50, Loss: 3192.0230244100094\n",
      "Epoch 10/50, Loss: 3190.5923019349575\n",
      "Epoch 11/50, Loss: 3189.697148323059\n",
      "Epoch 12/50, Loss: 3187.129758745432\n",
      "Epoch 13/50, Loss: 3184.9911658465862\n",
      "Epoch 14/50, Loss: 3184.9546769559383\n",
      "Epoch 15/50, Loss: 3184.019710868597\n",
      "Epoch 16/50, Loss: 3183.9676348268986\n",
      "Epoch 17/50, Loss: 3181.434725135565\n",
      "Epoch 18/50, Loss: 3181.268100142479\n",
      "Epoch 19/50, Loss: 3178.8603362739086\n",
      "Epoch 20/50, Loss: 3176.5032584369183\n",
      "Epoch 21/50, Loss: 3174.0020075440407\n",
      "Epoch 22/50, Loss: 3172.442873120308\n",
      "Epoch 23/50, Loss: 3170.2709589600563\n",
      "Epoch 24/50, Loss: 3167.2356988191605\n",
      "Epoch 25/50, Loss: 3163.2466747760773\n",
      "Epoch 26/50, Loss: 3160.235849380493\n",
      "Epoch 27/50, Loss: 3157.401012778282\n",
      "Epoch 28/50, Loss: 3155.81391620636\n",
      "Epoch 29/50, Loss: 3154.6678814291954\n",
      "Epoch 30/50, Loss: 3153.270157933235\n",
      "Epoch 31/50, Loss: 3152.0375125408173\n",
      "Epoch 32/50, Loss: 3149.1826129853725\n",
      "Epoch 33/50, Loss: 3149.0340922772884\n",
      "Epoch 34/50, Loss: 3147.9201694130898\n",
      "Epoch 35/50, Loss: 3146.5886808633804\n",
      "Epoch 36/50, Loss: 3146.3337756097317\n",
      "Epoch 37/50, Loss: 3145.937994837761\n",
      "Epoch 38/50, Loss: 3144.23573538661\n",
      "Epoch 39/50, Loss: 3143.859791278839\n",
      "Epoch 40/50, Loss: 3143.041027188301\n",
      "Epoch 41/50, Loss: 3143.0817979574203\n",
      "Epoch 42/50, Loss: 3141.215103149414\n",
      "Epoch 43/50, Loss: 3141.534513771534\n",
      "Epoch 44/50, Loss: 3139.5992555618286\n",
      "Epoch 45/50, Loss: 3138.9017379283905\n",
      "Epoch 46/50, Loss: 3139.047589302063\n",
      "Epoch 47/50, Loss: 3134.6020649671555\n",
      "Epoch 48/50, Loss: 3134.8247643113136\n",
      "Epoch 49/50, Loss: 3132.8643520772457\n",
      "Epoch 50/50, Loss: 3132.6437705159187\n",
      "Accuracy: 0.638467336683417\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat = []\n",
    "X_test_concat = []\n",
    "\n",
    "for review in X_train_w2v_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor = torch.tensor(X_train_concat, dtype=torch.float32)\n",
    "X_test_concat_tensor = torch.tensor(X_test_concat, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat = TensorDataset(X_train_concat_tensor, y_train_tensor)\n",
    "test_dataset_concat = TensorDataset(X_test_concat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_concat, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_concat, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Assuming 2 classes for sentiment analysis\n",
    "model_concat = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat, test_loader_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Binary Classification for the Word2Vec Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3314.3547974824905\n",
      "Epoch 2/50, Loss: 3242.0267139971256\n",
      "Epoch 3/50, Loss: 3224.446726948023\n",
      "Epoch 4/50, Loss: 3211.0086663365364\n",
      "Epoch 5/50, Loss: 3202.2065628170967\n",
      "Epoch 6/50, Loss: 3195.6309146285057\n",
      "Epoch 7/50, Loss: 3191.9821802079678\n",
      "Epoch 8/50, Loss: 3187.1544785499573\n",
      "Epoch 9/50, Loss: 3182.3843071758747\n",
      "Epoch 10/50, Loss: 3175.2804859280586\n",
      "Epoch 11/50, Loss: 3170.7927663326263\n",
      "Epoch 12/50, Loss: 3166.9846815764904\n",
      "Epoch 13/50, Loss: 3163.516598433256\n",
      "Epoch 14/50, Loss: 3160.2638128995895\n",
      "Epoch 15/50, Loss: 3157.704405605793\n",
      "Epoch 16/50, Loss: 3154.710099965334\n",
      "Epoch 17/50, Loss: 3152.761065542698\n",
      "Epoch 18/50, Loss: 3149.1145381629467\n",
      "Epoch 19/50, Loss: 3148.310146123171\n",
      "Epoch 20/50, Loss: 3146.2868282794952\n",
      "Epoch 21/50, Loss: 3143.970229446888\n",
      "Epoch 22/50, Loss: 3143.9778038859367\n",
      "Epoch 23/50, Loss: 3141.7837656736374\n",
      "Epoch 24/50, Loss: 3141.3118644058704\n",
      "Epoch 25/50, Loss: 3139.039125174284\n",
      "Epoch 26/50, Loss: 3137.4842279553413\n",
      "Epoch 27/50, Loss: 3137.0150220394135\n",
      "Epoch 28/50, Loss: 3135.701799094677\n",
      "Epoch 29/50, Loss: 3133.949146270752\n",
      "Epoch 30/50, Loss: 3133.1597623229027\n",
      "Epoch 31/50, Loss: 3132.5265150666237\n",
      "Epoch 32/50, Loss: 3131.253515601158\n",
      "Epoch 33/50, Loss: 3131.8348763287067\n",
      "Epoch 34/50, Loss: 3131.656037300825\n",
      "Epoch 35/50, Loss: 3128.4385494589806\n",
      "Epoch 36/50, Loss: 3127.106327176094\n",
      "Epoch 37/50, Loss: 3125.7150690853596\n",
      "Epoch 38/50, Loss: 3126.908812582493\n",
      "Epoch 39/50, Loss: 3124.813260704279\n",
      "Epoch 40/50, Loss: 3124.156830430031\n",
      "Epoch 41/50, Loss: 3124.5963036715984\n",
      "Epoch 42/50, Loss: 3122.7433320581913\n",
      "Epoch 43/50, Loss: 3121.6939577162266\n",
      "Epoch 44/50, Loss: 3121.4175332188606\n",
      "Epoch 45/50, Loss: 3120.565030038357\n",
      "Epoch 46/50, Loss: 3121.519037991762\n",
      "Epoch 47/50, Loss: 3118.65090867877\n",
      "Epoch 48/50, Loss: 3118.485656142235\n",
      "Epoch 49/50, Loss: 3118.358697682619\n",
      "Epoch 50/50, Loss: 3117.938712745905\n",
      "Accuracy: 0.6419346733668342\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat = []\n",
    "X_test_concat = []\n",
    "\n",
    "for review in X_train_w2v_own_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v_own_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor = torch.tensor(X_train_concat, dtype=torch.float32)\n",
    "X_test_concat_tensor = torch.tensor(X_test_concat, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat = TensorDataset(X_train_concat_tensor, y_train_tensor)\n",
    "test_dataset_concat = TensorDataset(X_test_concat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_concat, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_concat, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Assuming 2 classes for sentiment analysis\n",
    "model_concat = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat, test_loader_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison between MLPs and simpler models like perceptrons and Support Vector Machines (SVMs) highlights the trade-offs between model complexity and performance. While the MLPs have definetly over-performed the simple models in terms of the testing accuracy. However the training time between the MLPs and Simple Models is significantly higher, so that is a negative of the MLPs. Perceptrons and SVMs, while less complex, are easier to interpret and computationally more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Binary Classification using the Word2Vec Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am splitting the dataset again from the initial data, and removing the sentiment values with class 3, as we are doing a binary classification. and encoding the class labels fro 1 and 2 to  and 1 as the class labels have to start from '0'.\n",
    "\n",
    "I am using a CNN, with the input size as 300 features from the Word2Vec feature vectors and the output size to be 2 as we are working on a binary classification.\n",
    "\n",
    "I am also printing the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining features and targets for training dataset\n",
    "train_data = pd.concat([X_train_nltk_preprocessed, y_train], axis=1)\n",
    "train_data_filtered = train_data[train_data['sentiment'] != 3]\n",
    "X_train_binary = train_data_filtered['review_body']\n",
    "y_train_binary = train_data_filtered['sentiment']\n",
    "\n",
    "# Joining features and targets for testing dataset\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data_filtered = test_data[test_data['sentiment'] != 3]\n",
    "X_test_binary = test_data_filtered['review_body']\n",
    "y_test_binary = test_data_filtered['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "def preprocess_text(text, max_length=50):\n",
    "    tokens = text.split()[:max_length]  # Limit maximum review length\n",
    "    padded_tokens = tokens + ['<PAD>'] * (max_length - len(tokens))  # Pad shorter reviews\n",
    "    return padded_tokens\n",
    "\n",
    "# Convert text data into Word2Vec vectors\n",
    "def text_to_vectors(texts, wv, max_length=50):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text, max_length)\n",
    "        vector = [wv[word] if word in wv else np.zeros(wv.vector_size) for word in tokens]\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Prepare data\n",
    "X_train_vectors = text_to_vectors(X_train_binary, wv)\n",
    "X_test_vectors = text_to_vectors(X_test_binary, wv)\n",
    "\n",
    "# Convert labels to binary format (class 1 and class 2)\n",
    "y_train_binary = np.where(y_train_binary == 1, 0, 1)\n",
    "y_test_binary = np.where(y_test_binary == 1, 0, 1)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_vectors, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_vectors, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(50, 10, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(10 * 12, output_size)  # Adjust the input size for the linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 12)  # Adjust the reshaping operation\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.softmax(x, dim=1)\n",
    "\n",
    "# Train the CNN model\n",
    "def train_cnn(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute input dimensions for Conv1D\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_cnn(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute input dimensions for Conv1D\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2385.407571732998\n",
      "Epoch 2/10, Loss: 2231.0871135890484\n",
      "Epoch 3/10, Loss: 2179.0149119198322\n",
      "Epoch 4/10, Loss: 2136.2539499402046\n",
      "Epoch 5/10, Loss: 2105.5084967315197\n",
      "Epoch 6/10, Loss: 2078.3546420931816\n",
      "Epoch 7/10, Loss: 2060.8316709697247\n",
      "Epoch 8/10, Loss: 2042.662799268961\n",
      "Epoch 9/10, Loss: 2027.9348596930504\n",
      "Epoch 10/10, Loss: 2012.1750220358372\n",
      "Accuracy: 0.7983668341708543\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train_vectors.shape[2]  # Size of input features\n",
    "output_size = 2  # Binary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Binary Classification using the Word2Vec Custom Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining features and targets for training dataset\n",
    "train_data = pd.concat([X_train_nltk_preprocessed, y_train], axis=1)\n",
    "train_data_filtered = train_data[train_data['sentiment'] != 3]\n",
    "X_train_binary = train_data_filtered['review_body']\n",
    "y_train_binary = train_data_filtered['sentiment']\n",
    "\n",
    "# Joining features and targets for testing dataset\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data_filtered = test_data[test_data['sentiment'] != 3]\n",
    "X_test_binary = test_data_filtered['review_body']\n",
    "y_test_binary = test_data_filtered['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train the Word2Vec model\n",
    "model_own = Word2Vec(X_train_nltk_preprocessed_new, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train_vectors = text_to_vectors(X_train_binary, model_own.wv)\n",
    "X_test_vectors = text_to_vectors(X_test_binary, model_own.wv)\n",
    "\n",
    "# Convert labels to binary format (class 1 and class 2)\n",
    "y_train_binary = np.where(y_train_binary == 1, 0, 1)\n",
    "y_test_binary = np.where(y_test_binary == 1, 0, 1)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_vectors, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_vectors, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2332.43872615695\n",
      "Epoch 2/10, Loss: 2261.077203631401\n",
      "Epoch 3/10, Loss: 2236.7446866333485\n",
      "Epoch 4/10, Loss: 2215.3790468871593\n",
      "Epoch 5/10, Loss: 2202.6864687800407\n",
      "Epoch 6/10, Loss: 2187.498899549246\n",
      "Epoch 7/10, Loss: 2177.059113651514\n",
      "Epoch 8/10, Loss: 2169.3832735419273\n",
      "Epoch 9/10, Loss: 2164.6578074991703\n",
      "Epoch 10/10, Loss: 2163.1868684887886\n",
      "Accuracy: 0.7752763819095477\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train_tensor.shape[2]  # Size of input features\n",
    "output_size = 2  # Binary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Ternary Classification with Pre-Trained Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using the dataset again from the initial data, and encoding the class labels from 1, 2 and 3 to 0, 1 and 2 as the class labels have to start from '0'.\n",
    "\n",
    "I am using a CNN, with the input size as 300 features from the Word2Vec feature vectors and the output size to be 3 as we are working on a Ternary classification.\n",
    "\n",
    "I am also printing the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining features and targets for training dataset\n",
    "train_data = pd.concat([X_train_nltk_preprocessed, y_train], axis=1)\n",
    "X_train = train_data['review_body']\n",
    "y_train = train_data['sentiment']\n",
    "\n",
    "# Joining features and targets for testing dataset\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "X_test = test_data['review_body']\n",
    "y_test = test_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "def preprocess_text(text, max_length=50):\n",
    "    tokens = text.split()[:max_length]  # Limit maximum review length\n",
    "    padded_tokens = tokens + ['<PAD>'] * (max_length - len(tokens))  # Pad shorter reviews\n",
    "    return padded_tokens\n",
    "\n",
    "# Convert text data into Word2Vec vectors\n",
    "def text_to_vectors(texts, wv, max_length=50):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text, max_length)\n",
    "        vector = [wv[word] if word in wv else np.zeros(wv.vector_size) for word in tokens]\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Prepare data\n",
    "X_train_vectors = text_to_vectors(X_train, wv)\n",
    "X_test_vectors = text_to_vectors(X_test, wv)\n",
    "\n",
    "# Convert labels to ternary format (classes 1, 2, and 3)\n",
    "y_train_ternary = np.array(y_train)  # Convert Pandas Series to NumPy array\n",
    "y_test_ternary = np.array(y_test)  # Convert Pandas Series to NumPy array\n",
    "\n",
    "# Convert labels to the range [0, 1, 2]\n",
    "y_train_ternary = y_train_ternary - 1\n",
    "y_test_ternary = y_test_ternary - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_vectors, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_ternary, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_vectors, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_ternary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for three classes\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(50, 10, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(10 * 12, output_size)  # Adjust the input size for the linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 12)  # Adjust the reshaping operation\n",
    "        x = self.fc(x)\n",
    "        return nn.functional.softmax(x, dim=1)\n",
    "\n",
    "# Train the CNN model for three classes\n",
    "def train_cnn(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute input dimensions for Conv1D\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluation for three classes\n",
    "def evaluate_cnn(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute input dimensions for Conv1D\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct / total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5506.197615265846\n",
      "Epoch 2/10, Loss: 5268.540997862816\n",
      "Epoch 3/10, Loss: 5184.202219247818\n",
      "Epoch 4/10, Loss: 5130.5243673324585\n",
      "Epoch 5/10, Loss: 5083.251011490822\n",
      "Epoch 6/10, Loss: 5039.066096842289\n",
      "Epoch 7/10, Loss: 5000.43970990181\n",
      "Epoch 8/10, Loss: 4974.9873413443565\n",
      "Epoch 9/10, Loss: 4947.3433557748795\n",
      "Epoch 10/10, Loss: 4927.723435997963\n",
      "Accuracy: 0.63098\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for ternary classification\n",
    "input_size = X_train_vectors.shape[2]  # Size of input features\n",
    "output_size = 3  # Ternary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Ternary Classification using the Word2Vec Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining features and targets for training dataset\n",
    "train_data = pd.concat([X_train_nltk_preprocessed, y_train], axis=1)\n",
    "X_train = train_data['review_body']\n",
    "y_train = train_data['sentiment']\n",
    "\n",
    "# Joining features and targets for testing dataset\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "X_test = test_data['review_body']\n",
    "y_test = test_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train the Word2Vec model\n",
    "model_own = Word2Vec(X_train_nltk_preprocessed_new, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "def preprocess_text(text, max_length=50):\n",
    "    tokens = text.split()[:max_length]  # Limit maximum review length\n",
    "    padded_tokens = tokens + ['<PAD>'] * (max_length - len(tokens))  # Pad shorter reviews\n",
    "    return padded_tokens\n",
    "\n",
    "# Convert text data into Word2Vec vectors\n",
    "def text_to_vectors(texts, wv, max_length=50):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text, max_length)\n",
    "        vector = [wv[word] if word in wv else np.zeros(wv.vector_size) for word in tokens]\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Prepare data\n",
    "X_train_vectors = text_to_vectors(X_train, model_own.wv)\n",
    "X_test_vectors = text_to_vectors(X_test, model_own.wv)\n",
    "\n",
    "# Convert labels to ternary format (classes 1, 2, and 3)\n",
    "y_train_ternary = np.array(y_train)  # Convert Pandas Series to NumPy array\n",
    "y_test_ternary = np.array(y_test)  # Convert Pandas Series to NumPy array\n",
    "\n",
    "# Convert labels to the range [0, 1, 2]\n",
    "y_train_ternary = y_train_ternary - 1\n",
    "y_test_ternary = y_test_ternary - 1\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_vectors, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_ternary, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_vectors, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_ternary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5383.035078883171\n",
      "Epoch 2/10, Loss: 5289.389633178711\n",
      "Epoch 3/10, Loss: 5247.818126320839\n",
      "Epoch 4/10, Loss: 5223.400608718395\n",
      "Epoch 5/10, Loss: 5203.720783531666\n",
      "Epoch 6/10, Loss: 5194.734705924988\n",
      "Epoch 7/10, Loss: 5186.168330729008\n",
      "Epoch 8/10, Loss: 5181.665784597397\n",
      "Epoch 9/10, Loss: 5175.856912493706\n",
      "Epoch 10/10, Loss: 5178.5594519376755\n",
      "Accuracy: 0.62016\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for ternary classification\n",
    "input_size = X_train_vectors.shape[2]  # Size of input features\n",
    "output_size = 3  # Ternary classification\n",
    "model = CNN(input_size, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_cnn(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_cnn(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. PyTorch Official Documentation: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "2. PyTorch Tutorials on GitHub: https://github.com/pytorch/tutorials\n",
    "\n",
    "3. Deep Learning with PyTorch Book: https://www.manning.com/books/deep-learning-with-pytorch\n",
    "\n",
    "4. PyTorch Lightning GitHub Repository: https://github.com/PyTorchLightning/pytorch-lightning\n",
    "\n",
    "5. Stanford CS231n Course Website: http://cs231n.stanford.edu/\n",
    "\n",
    "6. Fast.ai: https://www.fast.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
