{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Anne Sai Venkata Naga Saketh <br>\n",
    "USC ID: 3725520208 <br>\n",
    "USC Email: annes@usc.edu <br>\n",
    "<b> NLP HW2 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sakethanne/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sakethanne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import re            # Regular expressions for text processing\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk          # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet data\n",
    "nltk.download('stopwords')   # Download StopWords data\n",
    "\n",
    "import warnings      # To handle warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for the remainder of the code\n",
    "warnings.filterwarnings(\"default\")  # Set warnings back to default behavior\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_12747/1150709939.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  full_data = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n",
      "Skipping line 20773: expected 15 fields, saw 22\n",
      "Skipping line 39834: expected 15 fields, saw 22\n",
      "Skipping line 52957: expected 15 fields, saw 22\n",
      "Skipping line 54540: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 80276: expected 15 fields, saw 22\n",
      "Skipping line 96168: expected 15 fields, saw 22\n",
      "Skipping line 96866: expected 15 fields, saw 22\n",
      "Skipping line 98175: expected 15 fields, saw 22\n",
      "Skipping line 112539: expected 15 fields, saw 22\n",
      "Skipping line 119377: expected 15 fields, saw 22\n",
      "Skipping line 120065: expected 15 fields, saw 22\n",
      "Skipping line 124703: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 134024: expected 15 fields, saw 22\n",
      "Skipping line 153938: expected 15 fields, saw 22\n",
      "Skipping line 156225: expected 15 fields, saw 22\n",
      "Skipping line 168603: expected 15 fields, saw 22\n",
      "Skipping line 187002: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 200397: expected 15 fields, saw 22\n",
      "Skipping line 203809: expected 15 fields, saw 22\n",
      "Skipping line 207680: expected 15 fields, saw 22\n",
      "Skipping line 223421: expected 15 fields, saw 22\n",
      "Skipping line 244032: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 270329: expected 15 fields, saw 22\n",
      "Skipping line 276484: expected 15 fields, saw 22\n",
      "Skipping line 304755: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 379449: expected 15 fields, saw 22\n",
      "Skipping line 386191: expected 15 fields, saw 22\n",
      "Skipping line 391811: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 414348: expected 15 fields, saw 22\n",
      "Skipping line 414773: expected 15 fields, saw 22\n",
      "Skipping line 417572: expected 15 fields, saw 22\n",
      "Skipping line 419496: expected 15 fields, saw 22\n",
      "Skipping line 430528: expected 15 fields, saw 22\n",
      "Skipping line 442230: expected 15 fields, saw 22\n",
      "Skipping line 450931: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 465377: expected 15 fields, saw 22\n",
      "Skipping line 467685: expected 15 fields, saw 22\n",
      "Skipping line 485055: expected 15 fields, saw 22\n",
      "Skipping line 487220: expected 15 fields, saw 22\n",
      "Skipping line 496076: expected 15 fields, saw 22\n",
      "Skipping line 512269: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 529505: expected 15 fields, saw 22\n",
      "Skipping line 531286: expected 15 fields, saw 22\n",
      "Skipping line 535424: expected 15 fields, saw 22\n",
      "Skipping line 569898: expected 15 fields, saw 22\n",
      "Skipping line 586293: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 593880: expected 15 fields, saw 22\n",
      "Skipping line 599274: expected 15 fields, saw 22\n",
      "Skipping line 607961: expected 15 fields, saw 22\n",
      "Skipping line 612413: expected 15 fields, saw 22\n",
      "Skipping line 615913: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 677580: expected 15 fields, saw 22\n",
      "Skipping line 687191: expected 15 fields, saw 22\n",
      "Skipping line 710819: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 728692: expected 15 fields, saw 22\n",
      "Skipping line 730216: expected 15 fields, saw 22\n",
      "Skipping line 758397: expected 15 fields, saw 22\n",
      "Skipping line 760061: expected 15 fields, saw 22\n",
      "Skipping line 768935: expected 15 fields, saw 22\n",
      "Skipping line 769483: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 822725: expected 15 fields, saw 22\n",
      "Skipping line 823621: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 857041: expected 15 fields, saw 22\n",
      "Skipping line 857320: expected 15 fields, saw 22\n",
      "Skipping line 858565: expected 15 fields, saw 22\n",
      "Skipping line 860629: expected 15 fields, saw 22\n",
      "Skipping line 864033: expected 15 fields, saw 22\n",
      "Skipping line 868673: expected 15 fields, saw 22\n",
      "Skipping line 869189: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 938605: expected 15 fields, saw 22\n",
      "Skipping line 940100: expected 15 fields, saw 22\n",
      "Skipping line 975137: expected 15 fields, saw 22\n",
      "Skipping line 976314: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 985597: expected 15 fields, saw 22\n",
      "Skipping line 990873: expected 15 fields, saw 22\n",
      "Skipping line 991806: expected 15 fields, saw 22\n",
      "Skipping line 1019808: expected 15 fields, saw 22\n",
      "Skipping line 1021526: expected 15 fields, saw 22\n",
      "Skipping line 1023905: expected 15 fields, saw 22\n",
      "Skipping line 1044207: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1084683: expected 15 fields, saw 22\n",
      "Skipping line 1093288: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1136430: expected 15 fields, saw 22\n",
      "Skipping line 1139815: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1179821: expected 15 fields, saw 22\n",
      "Skipping line 1195351: expected 15 fields, saw 22\n",
      "Skipping line 1202007: expected 15 fields, saw 22\n",
      "Skipping line 1224868: expected 15 fields, saw 22\n",
      "Skipping line 1232490: expected 15 fields, saw 22\n",
      "Skipping line 1238697: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1258654: expected 15 fields, saw 22\n",
      "Skipping line 1279948: expected 15 fields, saw 22\n",
      "Skipping line 1294360: expected 15 fields, saw 22\n",
      "Skipping line 1302240: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1413654: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1687095: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1805966: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1892134: expected 15 fields, saw 22\n",
      "\n",
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_12747/1150709939.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_data = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Reading the data from the tsv (Amazon Kitchen dataset) file as a Pandas frame\n",
    "full_data = pd.read_csv(\"./amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract only Review and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0                US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
      "1                US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
      "2                US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
      "3                US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
      "4                US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
      "...             ...          ...             ...         ...             ...   \n",
      "2640249          US     53005790   RLI7EI10S7SN0  B00000DM9M       223408988   \n",
      "2640250          US     52188548  R1F3SRK9MHE6A3  B00000DM9M       223408988   \n",
      "2640251          US     52090046  R23V0C4NRJL8EM  0807865001       307284585   \n",
      "2640252          US     52503173  R13ZAE1ATEUC1T  1572313188       870359649   \n",
      "2640253          US     52585611   RE8J5O2GY04NN  1572313188       870359649   \n",
      "\n",
      "                                             product_title product_category  \\\n",
      "0           Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
      "1                Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
      "2        Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
      "3        AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
      "4        Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
      "...                                                    ...              ...   \n",
      "2640249                 PalmOne III Leather Belt Clip Case  Office Products   \n",
      "2640250                 PalmOne III Leather Belt Clip Case  Office Products   \n",
      "2640251                  Gods and Heroes of Ancient Greece  Office Products   \n",
      "2640252  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
      "2640253  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
      "\n",
      "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0                 5            0.0          0.0    N                 Y   \n",
      "1                 5            0.0          1.0    N                 Y   \n",
      "2                 5            0.0          0.0    N                 Y   \n",
      "3                 1            2.0          3.0    N                 Y   \n",
      "4                 4            0.0          0.0    N                 Y   \n",
      "...             ...            ...          ...  ...               ...   \n",
      "2640249           4           26.0         26.0    N                 N   \n",
      "2640250           4           18.0         18.0    N                 N   \n",
      "2640251           4            9.0         16.0    N                 N   \n",
      "2640252           5            0.0          0.0    N                 N   \n",
      "2640253           5            0.0          0.0    N                 N   \n",
      "\n",
      "                                           review_headline  \\\n",
      "0                                               Five Stars   \n",
      "1        Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
      "2                            but I am sure I will like it.   \n",
      "3        and the shredder was dirty and the bin was par...   \n",
      "4                                               Four Stars   \n",
      "...                                                    ...   \n",
      "2640249  Great value! A must if you hate to carry thing...   \n",
      "2640250          Attaches the Palm Pilot like an appendage   \n",
      "2640251  Excellent information, pictures and stories, I...   \n",
      "2640252                                         class text   \n",
      "2640253                                 Microsoft's Finest   \n",
      "\n",
      "                                               review_body review_date  \n",
      "0                                           Great product.  2015-08-31  \n",
      "1        What's to say about this commodity item except...  2015-08-31  \n",
      "2          Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
      "3        Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
      "4                          Gorgeous colors and easy to use  2015-08-31  \n",
      "...                                                    ...         ...  \n",
      "2640249  I can't live anymore whithout my Palm III. But...  1998-12-07  \n",
      "2640250  Although the Palm Pilot is thin and compact it...  1998-11-30  \n",
      "2640251  This book had a lot of great content without b...  1998-10-15  \n",
      "2640252  I am teaching a course in Excel and am using t...  1998-08-22  \n",
      "2640253  A very comprehensive layout of exactly how Vis...  1998-07-15  \n",
      "\n",
      "[2640254 rows x 15 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_12747/79198575.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['star_rating'] = pd.to_numeric(full_data['star_rating'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Printing the data frame that contains the entire dataset from the tsv file\n",
    "print(full_data)\n",
    "\n",
    "# Keep only the Reviews and Ratings fields from the full data\n",
    "df = full_data[['review_body', 'star_rating']]\n",
    "\n",
    "# Converting 'star_rating' to numeric values\n",
    "df['star_rating'] = pd.to_numeric(full_data['star_rating'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a Balanced Data Set and prepare a Testing and Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_12747/2411086817.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the Dataset has been balanced out:\n",
      "\n",
      "Star_rating  Count\n",
      "5    50000\n",
      "1    50000\n",
      "4    50000\n",
      "2    50000\n",
      "3    50000\n",
      "Name: star_rating, dtype: int64\n",
      "Checking if the Sentiments has been balanced out:\n",
      "\n",
      "Sentiment  Count\n",
      "1    100000\n",
      "2    100000\n",
      "3     50000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Printing the Test and the Training set data sizes\n",
      "Train dataset size: 200000\n",
      "Test dataset size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in 'star_rating' column\n",
    "unique_ratings = df['star_rating'].unique()\n",
    "# print(\"Unique ratings:\", unique_ratings)\n",
    "\n",
    "# Convert 'star_rating' column to integer, handling errors\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in 'star_rating' column\n",
    "df = df.dropna(subset=['star_rating'])\n",
    "\n",
    "# Convert 'star_rating' column to integer\n",
    "df['star_rating'] = df['star_rating'].astype(int)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        return review, rating\n",
    "\n",
    "# Build balanced dataset\n",
    "ratings = df['star_rating'].unique()\n",
    "balanced_data = pd.DataFrame(columns=df.columns)\n",
    "for rating in ratings:\n",
    "    subset = df[df['star_rating'] == rating]\n",
    "    if len(subset) >= 50000:\n",
    "        subset = subset.sample(n=50000, random_state=42)\n",
    "    balanced_data = pd.concat([balanced_data, subset])\n",
    "\n",
    "# Create ternary labels\n",
    "balanced_data['sentiment'] = np.where(balanced_data['star_rating'] > 3, 1, \n",
    "                                      np.where(balanced_data['star_rating'] < 3, 2, 3))\n",
    "\n",
    "print(\"Checking if the Dataset has been balanced out:\\n\")\n",
    "print(\"Star_rating  Count\")\n",
    "print(balanced_data['star_rating'].value_counts())\n",
    "\n",
    "print(\"Checking if the Sentiments has been balanced out:\\n\")\n",
    "print(\"Sentiment  Count\")\n",
    "print(balanced_data['sentiment'].value_counts())\n",
    "\n",
    "# Perform train-test split\n",
    "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define train and test datasets\n",
    "train_dataset = AmazonReviewsDataset(train_data['review_body'].values, train_data['sentiment'].values)\n",
    "test_dataset = AmazonReviewsDataset(test_data['review_body'].values, test_data['sentiment'].values)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print sizes of train and test datasets\n",
    "print(\"\\nPrinting the Test and the Training set data sizes\")\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Training and Testing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_data['review_body'],\n",
    "                                                    balanced_data['sentiment'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_12747/3895173410.py:75: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_12747/3895173410.py:75: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Printing the Average lenght of Reviews Before and After Cleaning====================\n",
      "\n",
      "Average Length of Reviews (Before Cleaning): 343 characters\n",
      "Average Length of Reviews (After Cleaning): 326 characters\n"
     ]
    }
   ],
   "source": [
    "# Define a contraction map\n",
    "CONTRACTION_MAP = {\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\"\n",
    "}\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    for contraction, expansion in CONTRACTION_MAP.items():\n",
    "        text = re.sub(contraction, expansion, text)\n",
    "    return text\n",
    "\n",
    "# Preprocess the reviews\n",
    "def preprocess_reviews(reviews):\n",
    "    # Convert to lowercase and handle NaN values\n",
    "    reviews = reviews.apply(lambda x: str(x).lower() if pd.notna(x) else '')\n",
    "    \n",
    "    # Remove HTML and URLs\n",
    "    reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "    reviews = reviews.apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "    # Remove non-alphabetical characters (excluding single quote)\n",
    "    reviews = reviews.apply(lambda x: re.sub(r'[^a-zA-Z\\s\\']', '', x))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    reviews = reviews.apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    # Perform contractions\n",
    "    reviews = reviews.apply(expand_contractions)\n",
    "\n",
    "    # Return the processed text of the review\n",
    "    return reviews\n",
    "\n",
    "# Preprocess the training set\n",
    "X_train_preprocessed = preprocess_reviews(X_train)\n",
    "\n",
    "# Print average length of reviews before and after cleaning\n",
    "avg_length_before = X_train.apply(lambda x: len(str(x))).mean()\n",
    "avg_length_after = X_train_preprocessed.apply(len).mean()\n",
    "print(\"===================Printing the Average lenght of Reviews Before and After Cleaning====================\")\n",
    "print(f\"\\nAverage Length of Reviews (Before Cleaning): {int(avg_length_before)} characters\")\n",
    "print(f\"Average Length of Reviews (After Cleaning): {int(avg_length_after)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Printing Sample Reviews Before and After Pre-processing =============\n",
      "\n",
      "Sample Review 149348 Before Pre-processing:\n",
      "very good fit replacement battery\n",
      "\n",
      "Sample Review 149348 After NLTK Pre-processing:\n",
      "good fit replacement battery\n",
      "\n",
      "Sample Review 931199 Before Pre-processing:\n",
      "i have not had it that long and it will not hold a charge actually will not charge even got a new battery and that is not the problem i will probably just have to trash it and get something else when it worked it was fine\n",
      "\n",
      "Sample Review 931199 After NLTK Pre-processing:\n",
      "long hold charge actually charge even got new battery problem probably trash get something else worked fine\n",
      "\n",
      "Sample Review 652920 Before Pre-processing:\n",
      "i am very pleased with the product and the price\n",
      "\n",
      "Sample Review 652920 After NLTK Pre-processing:\n",
      "pleased product price\n",
      "\n",
      "=================Printing the Average lenght of Reviews Before and After Pre-processing==================\n",
      "\n",
      "Average Length of Reviews (Before NLTK Processing): 326 characters\n",
      "Average Length of Reviews (After NLTK Processing): 201 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK's stopwords and WordNet lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to remove stop words and perform lemmatization\n",
    "def preprocess_nltk(review):\n",
    "    if pd.notna(review):\n",
    "        words = nltk.word_tokenize(str(review).lower())  # Convert to lowercase\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Preprocess the training set using NLTK\n",
    "X_train_nltk_preprocessed = X_train_preprocessed.apply(preprocess_nltk)\n",
    "\n",
    "# Print three sample reviews before and after NLTK preprocessing\n",
    "sample_reviews_indices = X_train_preprocessed.sample(3).index\n",
    "\n",
    "print(\"============ Printing Sample Reviews Before and After Pre-processing =============\")\n",
    "for index in sample_reviews_indices:\n",
    "    print(f\"\\nSample Review {index} Before Pre-processing:\")\n",
    "    print(X_train_preprocessed.loc[index])\n",
    "\n",
    "    print(f\"\\nSample Review {index} After NLTK Pre-processing:\")\n",
    "    print(X_train_nltk_preprocessed.loc[index])\n",
    "\n",
    "# Print average length of reviews before and after NLTK processing\n",
    "avg_length_before_nltk = X_train_preprocessed.apply(len).mean()\n",
    "avg_length_after_nltk = X_train_nltk_preprocessed.apply(len).mean()\n",
    "print(\"\\n=================Printing the Average lenght of Reviews Before and After Pre-processing==================\")\n",
    "print(f\"\\nAverage Length of Reviews (Before NLTK Processing): {int(avg_length_before_nltk)} characters\")\n",
    "print(f\"Average Length of Reviews (After NLTK Processing): {int(avg_length_after_nltk)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Score using the Word2Vec Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining features and targets for training dataset\n",
    "train_data = pd.concat([X_train_nltk_preprocessed, y_train], axis=1)\n",
    "train_data_filtered = train_data[train_data['sentiment'] != 3]\n",
    "X_train_binary = train_data_filtered['review_body']\n",
    "y_train_binary = train_data_filtered['sentiment']\n",
    "\n",
    "# Joining features and targets for testing dataset\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data_filtered = test_data[test_data['sentiment'] != 3]\n",
    "X_test_binary = test_data_filtered['review_body']\n",
    "y_test_binary = test_data_filtered['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = [('queen', 0.7118192911148071)]\n",
      "Excellent ~ Outstanding = [('oustanding', 0.750198483467102)]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract Word2Vec features for a given sentence\n",
    "def extract_word2vec_features(sentence, model, vector_size):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.key_to_index:\n",
    "            word_vectors.append(model.get_vector(word))\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no word vectors found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)  # Return average word vector\n",
    "\n",
    "# Examples to check semantic similarities\n",
    "example1 = wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", example1)\n",
    "\n",
    "example2 = wv.most_similar(positive=['excellent', 'outstanding'], topn=1)\n",
    "print(\"Excellent ~ Outstanding =\", example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Score using the Word2Vec model with the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200000it [00:21, 9387.12it/s] \n"
     ]
    }
   ],
   "source": [
    "X_train_nltk_preprocessed_new = []\n",
    "\n",
    "for e, k in tqdm(enumerate(X_train_nltk_preprocessed.to_list())):\n",
    "    try:\n",
    "        X_train_nltk_preprocessed_new.append(word_tokenize(k))\n",
    "    except:\n",
    "        pass    \n",
    "def get_doc_embedding(doc):\n",
    "    words = doc.lower().split()\n",
    "    return wv.get_mean_vector(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train the Word2Vec model\n",
    "model_own = Word2Vec(X_train_nltk_preprocessed_new, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman (Own Model) = [('excellent', 0.47700005769729614)]\n",
      "Excellent ~ Outstanding (Own Model) = [('superb', 0.8427599668502808)]\n",
      "\n",
      "Semantic similarity (Pretrained Model): [('queen', 0.7118192911148071)]\n",
      "Semantic similarity (Pretrained Model): [('oustanding', 0.750198483467102)]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract Word2Vec features for a given sentence using the trained model\n",
    "def extract_word2vec_features_own(sentence, model, vector_size):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv.key_to_index:\n",
    "            word_vectors.append(model.wv.get_vector(word))\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return zero vector if no word vectors found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)  # Return average word vector\n",
    "\n",
    "# Examples to check semantic similarities using your own model\n",
    "if 'good' in model_own.wv.key_to_index:\n",
    "    example1_own = model_own.wv.most_similar(positive=['good'], negative=['bad'], topn=1)\n",
    "    print(\"King - Man + Woman (Own Model) =\", example1_own)\n",
    "else:\n",
    "    print(\"'good' is not present in the vocabulary.\")\n",
    "\n",
    "if 'excellent' in model_own.wv.key_to_index and 'outstanding' in model_own.wv.key_to_index:\n",
    "    example2_own = model_own.wv.most_similar(positive=['excellent', 'outstanding'], topn=1)\n",
    "    print(\"Excellent ~ Outstanding (Own Model) =\", example2_own)\n",
    "else:\n",
    "    print(\"'excellent' and/or 'outstanding' are not present in the vocabulary.\")\n",
    "\n",
    "# Compare semantic similarities between pretrained and own models\n",
    "print(\"\\nSemantic similarity (Pretrained Model):\", example1)\n",
    "print(\"Semantic similarity (Pretrained Model):\", example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing vectors generated by pre-trained Word2Vec models and those trained on specific datasets reveals nuanced trade-offs. Pre-trained models, leveraging vast corpora, excel in capturing broad semantic similarities but may lack fine-grained domain specificity. Conversely, self-generated vectors, tailored to specific contexts, offer potential for domain-specific insights but require representative data and entail computational costs. Evaluating both models on relevant tasks like word similarity or downstream applications is crucial to determining which better encodes semantic relationships for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the TF-IDF Features from Dataset and train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train_tfidf: (160200, 112187)\n",
      "Shape of X_test_tfidf: (39800, 112187)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000000)\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_binary)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_binary.apply(preprocess_nltk))\n",
    "\n",
    "# Print the shape of the TF-IDF matrices\n",
    "print(f\"\\nShape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Single Layer Perceptron using TF-IDF Features: 0.8129396984924623\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=1000)\n",
    "\n",
    "perceptron.fit(X_train_tfidf, y_train_binary)\n",
    "y_pred = perceptron.predict(X_test_tfidf)\n",
    "accuracy_perceptron_tf_idf = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the Single Layer Perceptron using TF-IDF Features: {accuracy_perceptron_tf_idf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM using TF-IDF Features: 0.8603015075376884\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_train_tfidf, y_train_binary)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "accuracy_svm_tf_idf = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the SVM using TF-IDF Features: {accuracy_svm_tf_idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Pre-trained Features from Dataset and train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 160200/160200 [01:31<00:00, 1754.54it/s]\n",
      "100%|███████████████████████████████████| 39800/39800 [00:34<00:00, 1150.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160200, 300), (39800, 300))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v_binary = []\n",
    "X_test_w2v_binary = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_binary):\n",
    "    X_train_w2v_binary.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test_binary):\n",
    "    X_test_w2v_binary.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v_binary = np.array(X_train_w2v_binary)\n",
    "X_test_w2v_binary = np.array(X_test_w2v_binary)\n",
    "\n",
    "X_train_w2v_binary.shape, X_test_w2v_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_one_hot = to_categorical(y_train_binary)\n",
    "# y_test_one_hot = to_categorical(y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Single Layer Perceptron using Word2Vec Pretrained Features: 0.4999497487437186\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=1000)\n",
    "\n",
    "perceptron.fit(X_train_w2v_binary, y_train_binary)\n",
    "y_pred = perceptron.predict(X_test_w2v_binary)\n",
    "accuracy_perceptron_w2v_pre = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the Single Layer Perceptron using Word2Vec Pretrained Features: {accuracy_perceptron_w2v_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM using Word2Vec Pretrained Features: 0.6078391959798995\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_train_w2v_binary, y_train_binary)\n",
    "y_pred = svm.predict(X_test_w2v_binary)\n",
    "accuracy_svm_w2v_pre = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the SVM using Word2Vec Pretrained Features: {accuracy_svm_w2v_pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Custom Features from Dataset and train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 160200/160200 [01:23<00:00, 1923.91it/s]\n",
      "100%|███████████████████████████████████| 39800/39800 [00:30<00:00, 1294.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160200, 300), (39800, 300))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return model_own.wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v_own_binary = []\n",
    "X_test_w2v_own_binary = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_binary):\n",
    "    X_train_w2v_own_binary.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test_binary):\n",
    "    X_test_w2v_own_binary.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v_own_binary = np.array(X_train_w2v_own_binary)\n",
    "X_test_w2v_own_binary = np.array(X_test_w2v_own_binary)\n",
    "\n",
    "X_train_w2v_own_binary.shape, X_test_w2v_own_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Single Layer Perceptron using Word2Vec Custom Features: 0.5013065326633166\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=1000)\n",
    "\n",
    "perceptron.fit(X_train_w2v_own_binary, y_train_binary)\n",
    "y_pred = perceptron.predict(X_test_w2v_own_binary)\n",
    "accuracy_perceptron_w2v_own = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the Single Layer Perceptron using Word2Vec Custom Features: {accuracy_perceptron_w2v_own}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVM using Word2Vec Custom Features: 0.6078140703517588\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "svm.fit(X_train_w2v_own_binary, y_train_binary)\n",
    "y_pred = svm.predict(X_test_w2v_own_binary)\n",
    "accuracy_svm_w2v_own = accuracy_score(y_test_binary, y_pred)\n",
    "print(f\"Accuracy of the SVM using Word2Vec Custom Features: {accuracy_svm_w2v_own}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "y_train_np = np.array(y_train_binary)\n",
    "y_test_np = np.array(y_test_binary)\n",
    "\n",
    "# Convert labels to binary format (0 for class 1, 1 for class 2)\n",
    "y_train_binary = np.where(y_train_np == 1, 0, 1)\n",
    "y_test_binary = np.where(y_test_np == 1, 0, 1)\n",
    "\n",
    "# Define the MLP Model for binary classification\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(BinaryMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Training Loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Custom Features and train a Multi Layer Perceptron with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3329.3513650894165\n",
      "Epoch 2/50, Loss: 3256.3875443935394\n",
      "Epoch 3/50, Loss: 3229.9809458851814\n",
      "Epoch 4/50, Loss: 3217.924126148224\n",
      "Epoch 5/50, Loss: 3209.0161807239056\n",
      "Epoch 6/50, Loss: 3205.7984357476234\n",
      "Epoch 7/50, Loss: 3199.1441574692726\n",
      "Epoch 8/50, Loss: 3196.9826451539993\n",
      "Epoch 9/50, Loss: 3194.0523723959923\n",
      "Epoch 10/50, Loss: 3188.603025317192\n",
      "Epoch 11/50, Loss: 3181.285680025816\n",
      "Epoch 12/50, Loss: 3177.8896557092667\n",
      "Epoch 13/50, Loss: 3175.050920009613\n",
      "Epoch 14/50, Loss: 3171.7565844357014\n",
      "Epoch 15/50, Loss: 3168.9987232983112\n",
      "Epoch 16/50, Loss: 3168.3483400046825\n",
      "Epoch 17/50, Loss: 3164.3925160467625\n",
      "Epoch 18/50, Loss: 3164.463406264782\n",
      "Epoch 19/50, Loss: 3163.1284128427505\n",
      "Epoch 20/50, Loss: 3161.315751582384\n",
      "Epoch 21/50, Loss: 3158.9698126912117\n",
      "Epoch 22/50, Loss: 3154.1887883245945\n",
      "Epoch 23/50, Loss: 3152.3663188517094\n",
      "Epoch 24/50, Loss: 3149.1161662340164\n",
      "Epoch 25/50, Loss: 3148.362521737814\n",
      "Epoch 26/50, Loss: 3146.098133325577\n",
      "Epoch 27/50, Loss: 3145.707335203886\n",
      "Epoch 28/50, Loss: 3144.1715104281902\n",
      "Epoch 29/50, Loss: 3141.691593706608\n",
      "Epoch 30/50, Loss: 3141.611049056053\n",
      "Epoch 31/50, Loss: 3140.615506798029\n",
      "Epoch 32/50, Loss: 3139.319735199213\n",
      "Epoch 33/50, Loss: 3139.5823071300983\n",
      "Epoch 34/50, Loss: 3137.2002751529217\n",
      "Epoch 35/50, Loss: 3136.9074364602566\n",
      "Epoch 36/50, Loss: 3133.4298642277718\n",
      "Epoch 37/50, Loss: 3134.4370078742504\n",
      "Epoch 38/50, Loss: 3133.326915204525\n",
      "Epoch 39/50, Loss: 3131.73722794652\n",
      "Epoch 40/50, Loss: 3132.526018857956\n",
      "Epoch 41/50, Loss: 3130.696269840002\n",
      "Epoch 42/50, Loss: 3130.7745197713375\n",
      "Epoch 43/50, Loss: 3130.9283580482006\n",
      "Epoch 44/50, Loss: 3128.329988926649\n",
      "Epoch 45/50, Loss: 3128.7498826682568\n",
      "Epoch 46/50, Loss: 3127.7592453956604\n",
      "Epoch 47/50, Loss: 3127.438626766205\n",
      "Epoch 48/50, Loss: 3125.6548860371113\n",
      "Epoch 49/50, Loss: 3125.5090637803078\n",
      "Epoch 50/50, Loss: 3126.6139416992664\n",
      "Accuracy: 0.6323618090452261\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_own_binary = TensorDataset(torch.tensor(X_train_w2v_own_binary, dtype=torch.float32), torch.tensor(y_train_binary, dtype=torch.long))\n",
    "test_dataset_own_binary = TensorDataset(torch.tensor(X_test_w2v_own_binary, dtype=torch.float32), torch.tensor(y_test_binary, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_own_binary = DataLoader(train_dataset_own_binary, batch_size=32, shuffle=True)\n",
    "test_loader_own_binary = DataLoader(test_dataset_own_binary, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v_own_binary.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Binary classification (classes 1 and 2)\n",
    "binary_model_own_binary = BinaryMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(binary_model_own_binary.parameters(), lr=0.001)\n",
    "\n",
    "# Train the binary classification model\n",
    "num_epochs = 50\n",
    "train(binary_model_own_binary, train_loader_own_binary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the binary classification model\n",
    "evaluate(binary_model_own_binary, test_loader_own_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Pretrained Features and train a Multi Layer Perceptron with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3337.4741054177284\n",
      "Epoch 2/50, Loss: 3258.777542024851\n",
      "Epoch 3/50, Loss: 3229.996010273695\n",
      "Epoch 4/50, Loss: 3217.8045231997967\n",
      "Epoch 5/50, Loss: 3210.592474937439\n",
      "Epoch 6/50, Loss: 3203.112156122923\n",
      "Epoch 7/50, Loss: 3198.713766813278\n",
      "Epoch 8/50, Loss: 3194.7120011150837\n",
      "Epoch 9/50, Loss: 3190.2736302912235\n",
      "Epoch 10/50, Loss: 3186.84253308177\n",
      "Epoch 11/50, Loss: 3183.506317228079\n",
      "Epoch 12/50, Loss: 3179.2488149106503\n",
      "Epoch 13/50, Loss: 3176.3761309981346\n",
      "Epoch 14/50, Loss: 3173.972962588072\n",
      "Epoch 15/50, Loss: 3171.9340659677982\n",
      "Epoch 16/50, Loss: 3169.777444243431\n",
      "Epoch 17/50, Loss: 3168.372892022133\n",
      "Epoch 18/50, Loss: 3163.1078752577305\n",
      "Epoch 19/50, Loss: 3162.1231328547\n",
      "Epoch 20/50, Loss: 3159.290839523077\n",
      "Epoch 21/50, Loss: 3158.057747721672\n",
      "Epoch 22/50, Loss: 3155.1658323407173\n",
      "Epoch 23/50, Loss: 3152.201946377754\n",
      "Epoch 24/50, Loss: 3150.5851868391037\n",
      "Epoch 25/50, Loss: 3150.4111152887344\n",
      "Epoch 26/50, Loss: 3146.548693239689\n",
      "Epoch 27/50, Loss: 3144.7266596853733\n",
      "Epoch 28/50, Loss: 3143.214090913534\n",
      "Epoch 29/50, Loss: 3141.0874778330326\n",
      "Epoch 30/50, Loss: 3140.023465305567\n",
      "Epoch 31/50, Loss: 3139.697517067194\n",
      "Epoch 32/50, Loss: 3137.7254694998264\n",
      "Epoch 33/50, Loss: 3135.2764590978622\n",
      "Epoch 34/50, Loss: 3134.583773434162\n",
      "Epoch 35/50, Loss: 3134.4141440093517\n",
      "Epoch 36/50, Loss: 3132.659614354372\n",
      "Epoch 37/50, Loss: 3130.9721278250217\n",
      "Epoch 38/50, Loss: 3131.5353623330593\n",
      "Epoch 39/50, Loss: 3131.0710247159004\n",
      "Epoch 40/50, Loss: 3127.989776521921\n",
      "Epoch 41/50, Loss: 3127.455612242222\n",
      "Epoch 42/50, Loss: 3124.735635995865\n",
      "Epoch 43/50, Loss: 3126.1454136669636\n",
      "Epoch 44/50, Loss: 3125.187182545662\n",
      "Epoch 45/50, Loss: 3125.6143840253353\n",
      "Epoch 46/50, Loss: 3124.110282957554\n",
      "Epoch 47/50, Loss: 3122.59104347229\n",
      "Epoch 48/50, Loss: 3121.166494935751\n",
      "Epoch 49/50, Loss: 3122.3645515441895\n",
      "Epoch 50/50, Loss: 3121.5522217452526\n",
      "Accuracy: 0.6342462311557789\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_pre_binary = TensorDataset(torch.tensor(X_train_w2v_binary, dtype=torch.float32), torch.tensor(y_train_binary, dtype=torch.long))\n",
    "test_dataset_pre_binary = TensorDataset(torch.tensor(X_test_w2v_binary, dtype=torch.float32), torch.tensor(y_test_binary, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_pre_binary = DataLoader(train_dataset_pre_binary, batch_size=32, shuffle=True)\n",
    "test_loader_pre_binary = DataLoader(test_dataset_pre_binary, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v_binary.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Binary classification (classes 1 and 2)\n",
    "binary_model_pre_binary = BinaryMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(binary_model_pre_binary.parameters(), lr=0.001)\n",
    "\n",
    "# Train the binary classification model\n",
    "num_epochs = 50\n",
    "train(binary_model_pre_binary, train_loader_pre_binary, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the binary classification model\n",
    "evaluate(binary_model_pre_binary, test_loader_pre_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron with Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200000/200000 [02:11<00:00, 1520.43it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:42<00:00, 1168.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return model_own.wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v_own = []\n",
    "X_test_w2v_own = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_nltk_preprocessed):\n",
    "    X_train_w2v_own.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test):\n",
    "    X_test_w2v_own.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v_own = np.array(X_train_w2v_own)\n",
    "X_test_w2v_own = np.array(X_test_w2v_own)\n",
    "\n",
    "X_train_w2v_own.shape, X_test_w2v_own.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 200000/200000 [02:09<00:00, 1539.86it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:46<00:00, 1085.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 300), (50000, 300))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_doc_embedding(doc):\n",
    "    doc_str = ' '.join(doc)  # Join tokens within the document list to form a single string\n",
    "    words = doc_str.lower().split()  # Split the string into words\n",
    "    if not words:\n",
    "        return np.zeros(300)  # Return a zero vector if no words are present\n",
    "    else:\n",
    "        return wv.get_mean_vector(words)  # Compute the mean vector using Word2Vec model\n",
    "\n",
    "X_train_w2v = []\n",
    "X_test_w2v = []\n",
    "\n",
    "# Extract Word2Vec features for training data\n",
    "for e in tqdm(X_train_nltk_preprocessed):\n",
    "    X_train_w2v.append(get_doc_embedding(e))\n",
    "\n",
    "# Extract Word2Vec features for testing data\n",
    "for e in tqdm(X_test):\n",
    "    X_test_w2v.append(get_doc_embedding(e))\n",
    "    \n",
    "X_train_w2v = np.array(X_train_w2v)\n",
    "X_test_w2v = np.array(X_test_w2v)\n",
    "\n",
    "X_train_w2v.shape, X_test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Convert labels to integers ranging from 0 to 2\n",
    "y_train_encoded = [label - 1 for label in y_train_np]  # Assuming labels start from 1\n",
    "y_test_encoded = [label - 1 for label in y_test_np]  # Assuming labels start from 1\n",
    "\n",
    "# Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Training Loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Pre-trained Features and train a Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6507.37576764822\n",
      "Epoch 2/50, Loss: 6392.508574426174\n",
      "Epoch 3/50, Loss: 6363.861379146576\n",
      "Epoch 4/50, Loss: 6349.22602045536\n",
      "Epoch 5/50, Loss: 6339.5665445923805\n",
      "Epoch 6/50, Loss: 6336.605700433254\n",
      "Epoch 7/50, Loss: 6328.115510582924\n",
      "Epoch 8/50, Loss: 6318.499319434166\n",
      "Epoch 9/50, Loss: 6308.493044793606\n",
      "Epoch 10/50, Loss: 6300.7000160217285\n",
      "Epoch 11/50, Loss: 6296.519177913666\n",
      "Epoch 12/50, Loss: 6291.990942358971\n",
      "Epoch 13/50, Loss: 6287.559504747391\n",
      "Epoch 14/50, Loss: 6283.4482543468475\n",
      "Epoch 15/50, Loss: 6280.895238697529\n",
      "Epoch 16/50, Loss: 6277.094581604004\n",
      "Epoch 17/50, Loss: 6271.9096955657005\n",
      "Epoch 18/50, Loss: 6272.777599990368\n",
      "Epoch 19/50, Loss: 6267.2739517092705\n",
      "Epoch 20/50, Loss: 6268.0393434762955\n",
      "Epoch 21/50, Loss: 6265.1053104400635\n",
      "Epoch 22/50, Loss: 6263.362920105457\n",
      "Epoch 23/50, Loss: 6261.958479940891\n",
      "Epoch 24/50, Loss: 6258.91188788414\n",
      "Epoch 25/50, Loss: 6257.381070971489\n",
      "Epoch 26/50, Loss: 6257.880622267723\n",
      "Epoch 27/50, Loss: 6255.746350705624\n",
      "Epoch 28/50, Loss: 6253.599887430668\n",
      "Epoch 29/50, Loss: 6253.9086918234825\n",
      "Epoch 30/50, Loss: 6252.3901416659355\n",
      "Epoch 31/50, Loss: 6250.439228117466\n",
      "Epoch 32/50, Loss: 6249.6026957035065\n",
      "Epoch 33/50, Loss: 6247.110570907593\n",
      "Epoch 34/50, Loss: 6247.851949334145\n",
      "Epoch 35/50, Loss: 6245.809594869614\n",
      "Epoch 36/50, Loss: 6245.897197186947\n",
      "Epoch 37/50, Loss: 6244.276473701\n",
      "Epoch 38/50, Loss: 6243.094761967659\n",
      "Epoch 39/50, Loss: 6242.03718316555\n",
      "Epoch 40/50, Loss: 6241.444157242775\n",
      "Epoch 41/50, Loss: 6241.406098723412\n",
      "Epoch 42/50, Loss: 6238.389196157455\n",
      "Epoch 43/50, Loss: 6239.357841432095\n",
      "Epoch 44/50, Loss: 6238.450036406517\n",
      "Epoch 45/50, Loss: 6236.270182669163\n",
      "Epoch 46/50, Loss: 6236.088974773884\n",
      "Epoch 47/50, Loss: 6235.792271614075\n",
      "Epoch 48/50, Loss: 6235.394916713238\n",
      "Epoch 49/50, Loss: 6234.491183400154\n",
      "Epoch 50/50, Loss: 6235.155323922634\n",
      "Accuracy: 0.50134\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_pre = TensorDataset(torch.tensor(X_train_w2v, dtype=torch.float32), torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_dataset_pre = TensorDataset(torch.tensor(X_test_w2v, dtype=torch.float32), torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_pre = DataLoader(train_dataset_pre, batch_size=32, shuffle=True)\n",
    "test_loader_pre = DataLoader(test_dataset_pre, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_pre = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_pre.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_pre, train_loader_pre, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_pre, test_loader_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Word2Vec Custom Features and train a Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6506.918885469437\n",
      "Epoch 2/50, Loss: 6399.554023325443\n",
      "Epoch 3/50, Loss: 6377.080897450447\n",
      "Epoch 4/50, Loss: 6363.953673005104\n",
      "Epoch 5/50, Loss: 6355.022880017757\n",
      "Epoch 6/50, Loss: 6348.933548569679\n",
      "Epoch 7/50, Loss: 6341.789035379887\n",
      "Epoch 8/50, Loss: 6332.161166250706\n",
      "Epoch 9/50, Loss: 6324.765000879765\n",
      "Epoch 10/50, Loss: 6319.54182434082\n",
      "Epoch 11/50, Loss: 6313.848741412163\n",
      "Epoch 12/50, Loss: 6310.189614236355\n",
      "Epoch 13/50, Loss: 6307.424668550491\n",
      "Epoch 14/50, Loss: 6303.995420217514\n",
      "Epoch 15/50, Loss: 6300.692300200462\n",
      "Epoch 16/50, Loss: 6297.822930276394\n",
      "Epoch 17/50, Loss: 6295.129075586796\n",
      "Epoch 18/50, Loss: 6290.334086894989\n",
      "Epoch 19/50, Loss: 6288.532537519932\n",
      "Epoch 20/50, Loss: 6285.952550649643\n",
      "Epoch 21/50, Loss: 6283.689514100552\n",
      "Epoch 22/50, Loss: 6282.665510833263\n",
      "Epoch 23/50, Loss: 6279.689943790436\n",
      "Epoch 24/50, Loss: 6277.734576165676\n",
      "Epoch 25/50, Loss: 6277.1504472494125\n",
      "Epoch 26/50, Loss: 6274.926438689232\n",
      "Epoch 27/50, Loss: 6273.028074383736\n",
      "Epoch 28/50, Loss: 6271.970549881458\n",
      "Epoch 29/50, Loss: 6270.632764220238\n",
      "Epoch 30/50, Loss: 6267.112630546093\n",
      "Epoch 31/50, Loss: 6266.542896091938\n",
      "Epoch 32/50, Loss: 6266.572944045067\n",
      "Epoch 33/50, Loss: 6264.748705148697\n",
      "Epoch 34/50, Loss: 6263.826687514782\n",
      "Epoch 35/50, Loss: 6262.309298455715\n",
      "Epoch 36/50, Loss: 6261.045805573463\n",
      "Epoch 37/50, Loss: 6259.336055278778\n",
      "Epoch 38/50, Loss: 6258.479123532772\n",
      "Epoch 39/50, Loss: 6257.375009536743\n",
      "Epoch 40/50, Loss: 6255.745802223682\n",
      "Epoch 41/50, Loss: 6255.187527179718\n",
      "Epoch 42/50, Loss: 6254.629574358463\n",
      "Epoch 43/50, Loss: 6253.878288865089\n",
      "Epoch 44/50, Loss: 6253.453043043613\n",
      "Epoch 45/50, Loss: 6251.235209584236\n",
      "Epoch 46/50, Loss: 6250.922207176685\n",
      "Epoch 47/50, Loss: 6249.336077570915\n",
      "Epoch 48/50, Loss: 6249.735640406609\n",
      "Epoch 49/50, Loss: 6249.132206916809\n",
      "Epoch 50/50, Loss: 6246.522408723831\n",
      "Accuracy: 0.4954\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset_own = TensorDataset(torch.tensor(X_train_w2v_own, dtype=torch.float32), torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_dataset_own = TensorDataset(torch.tensor(X_test_w2v_own, dtype=torch.float32), torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_own = DataLoader(train_dataset_own, batch_size=32, shuffle=True)\n",
    "test_loader_own = DataLoader(test_dataset_own, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_w2v_own.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_own = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_own.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_own, train_loader_own, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_own, test_loader_own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Ternary Classification for the Word2Vec Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6493.512466669083\n",
      "Epoch 2/50, Loss: 6403.701088666916\n",
      "Epoch 3/50, Loss: 6375.310911118984\n",
      "Epoch 4/50, Loss: 6358.125028192997\n",
      "Epoch 5/50, Loss: 6349.976001679897\n",
      "Epoch 6/50, Loss: 6341.9753767848015\n",
      "Epoch 7/50, Loss: 6334.247457385063\n",
      "Epoch 8/50, Loss: 6327.67810690403\n",
      "Epoch 9/50, Loss: 6322.453595340252\n",
      "Epoch 10/50, Loss: 6319.002834439278\n",
      "Epoch 11/50, Loss: 6313.617429792881\n",
      "Epoch 12/50, Loss: 6308.798898279667\n",
      "Epoch 13/50, Loss: 6305.797322332859\n",
      "Epoch 14/50, Loss: 6304.67782998085\n",
      "Epoch 15/50, Loss: 6301.674203693867\n",
      "Epoch 16/50, Loss: 6299.178352296352\n",
      "Epoch 17/50, Loss: 6297.800688326359\n",
      "Epoch 18/50, Loss: 6294.289004981518\n",
      "Epoch 19/50, Loss: 6291.694235086441\n",
      "Epoch 20/50, Loss: 6287.282687723637\n",
      "Epoch 21/50, Loss: 6283.074702978134\n",
      "Epoch 22/50, Loss: 6279.768384814262\n",
      "Epoch 23/50, Loss: 6277.975959420204\n",
      "Epoch 24/50, Loss: 6275.396085143089\n",
      "Epoch 25/50, Loss: 6272.225798428059\n",
      "Epoch 26/50, Loss: 6270.133328974247\n",
      "Epoch 27/50, Loss: 6268.396264076233\n",
      "Epoch 28/50, Loss: 6266.673579752445\n",
      "Epoch 29/50, Loss: 6262.95459574461\n",
      "Epoch 30/50, Loss: 6260.0800649523735\n",
      "Epoch 31/50, Loss: 6259.666492938995\n",
      "Epoch 32/50, Loss: 6257.74016469717\n",
      "Epoch 33/50, Loss: 6256.309556782246\n",
      "Epoch 34/50, Loss: 6255.6956850886345\n",
      "Epoch 35/50, Loss: 6254.678238809109\n",
      "Epoch 36/50, Loss: 6254.006768524647\n",
      "Epoch 37/50, Loss: 6251.038646638393\n",
      "Epoch 38/50, Loss: 6250.473570227623\n",
      "Epoch 39/50, Loss: 6251.223726153374\n",
      "Epoch 40/50, Loss: 6249.018192529678\n",
      "Epoch 41/50, Loss: 6248.7280304431915\n",
      "Epoch 42/50, Loss: 6248.928659200668\n",
      "Epoch 43/50, Loss: 6246.899600863457\n",
      "Epoch 44/50, Loss: 6246.977191209793\n",
      "Epoch 45/50, Loss: 6245.18234193325\n",
      "Epoch 46/50, Loss: 6244.480374395847\n",
      "Epoch 47/50, Loss: 6243.9304512143135\n",
      "Epoch 48/50, Loss: 6243.194746673107\n",
      "Epoch 49/50, Loss: 6241.784166097641\n",
      "Epoch 50/50, Loss: 6240.836602330208\n",
      "Accuracy: 0.51018\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat_own = []\n",
    "X_test_concat_own = []\n",
    "\n",
    "for review in X_train_w2v_own:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat_own.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v_own:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat_own.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor_own = torch.tensor(X_train_concat_own, dtype=torch.float32)\n",
    "X_test_concat_tensor_own = torch.tensor(X_test_concat_own, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor_own = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor_own = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat_own = TensorDataset(X_train_concat_tensor_own, y_train_tensor_own)\n",
    "test_dataset_concat_own = TensorDataset(X_test_concat_tensor_own, y_test_tensor_own)\n",
    "\n",
    "train_loader_concat_own = DataLoader(train_dataset_concat_own, batch_size=32, shuffle=True)\n",
    "test_loader_concat_own = DataLoader(test_dataset_concat_own, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor_own.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_concat_own = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat_own.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat_own, train_loader_concat_own, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat_own, test_loader_concat_own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Ternary Classification for the Word2Vec Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6518.244131207466\n",
      "Epoch 2/50, Loss: 6420.93543356657\n",
      "Epoch 3/50, Loss: 6396.908332765102\n",
      "Epoch 4/50, Loss: 6374.263934910297\n",
      "Epoch 5/50, Loss: 6359.016069591045\n",
      "Epoch 6/50, Loss: 6348.8051480054855\n",
      "Epoch 7/50, Loss: 6340.967021346092\n",
      "Epoch 8/50, Loss: 6335.828918457031\n",
      "Epoch 9/50, Loss: 6331.106681883335\n",
      "Epoch 10/50, Loss: 6325.491824150085\n",
      "Epoch 11/50, Loss: 6323.024015903473\n",
      "Epoch 12/50, Loss: 6318.552897572517\n",
      "Epoch 13/50, Loss: 6316.903543651104\n",
      "Epoch 14/50, Loss: 6311.946495294571\n",
      "Epoch 15/50, Loss: 6306.8835836052895\n",
      "Epoch 16/50, Loss: 6303.869810521603\n",
      "Epoch 17/50, Loss: 6300.620277702808\n",
      "Epoch 18/50, Loss: 6298.602122366428\n",
      "Epoch 19/50, Loss: 6296.123710632324\n",
      "Epoch 20/50, Loss: 6293.452532947063\n",
      "Epoch 21/50, Loss: 6292.05080384016\n",
      "Epoch 22/50, Loss: 6289.058473885059\n",
      "Epoch 23/50, Loss: 6287.096545815468\n",
      "Epoch 24/50, Loss: 6284.060394287109\n",
      "Epoch 25/50, Loss: 6282.42000490427\n",
      "Epoch 26/50, Loss: 6279.200699985027\n",
      "Epoch 27/50, Loss: 6277.83346247673\n",
      "Epoch 28/50, Loss: 6275.924901843071\n",
      "Epoch 29/50, Loss: 6273.0268068909645\n",
      "Epoch 30/50, Loss: 6272.668272078037\n",
      "Epoch 31/50, Loss: 6267.948019146919\n",
      "Epoch 32/50, Loss: 6267.361307621002\n",
      "Epoch 33/50, Loss: 6266.943819582462\n",
      "Epoch 34/50, Loss: 6263.939589381218\n",
      "Epoch 35/50, Loss: 6262.966666519642\n",
      "Epoch 36/50, Loss: 6262.541936695576\n",
      "Epoch 37/50, Loss: 6260.608237504959\n",
      "Epoch 38/50, Loss: 6259.606810033321\n",
      "Epoch 39/50, Loss: 6258.574266970158\n",
      "Epoch 40/50, Loss: 6257.650945186615\n",
      "Epoch 41/50, Loss: 6256.729337096214\n",
      "Epoch 42/50, Loss: 6255.509446978569\n",
      "Epoch 43/50, Loss: 6254.568018198013\n",
      "Epoch 44/50, Loss: 6254.654593169689\n",
      "Epoch 45/50, Loss: 6253.391659080982\n",
      "Epoch 46/50, Loss: 6252.73227763176\n",
      "Epoch 47/50, Loss: 6251.535403013229\n",
      "Epoch 48/50, Loss: 6251.178747415543\n",
      "Epoch 49/50, Loss: 6250.417330741882\n",
      "Epoch 50/50, Loss: 6248.717987060547\n",
      "Accuracy: 0.50378\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat = []\n",
    "X_test_concat = []\n",
    "\n",
    "for review in X_train_w2v:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor = torch.tensor(X_train_concat, dtype=torch.float32)\n",
    "X_test_concat_tensor = torch.tensor(X_test_concat, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat = TensorDataset(X_train_concat_tensor, y_train_tensor)\n",
    "test_dataset_concat = TensorDataset(X_test_concat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_concat, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_concat, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 3  # Assuming 3 classes for sentiment analysis\n",
    "model_concat = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat, test_loader_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Binary Classification for the Word2Vec Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3342.113238632679\n",
      "Epoch 2/50, Loss: 3262.320921331644\n",
      "Epoch 3/50, Loss: 3237.025598526001\n",
      "Epoch 4/50, Loss: 3223.482433617115\n",
      "Epoch 5/50, Loss: 3216.778574824333\n",
      "Epoch 6/50, Loss: 3212.782517582178\n",
      "Epoch 7/50, Loss: 3209.696006208658\n",
      "Epoch 8/50, Loss: 3205.975496381521\n",
      "Epoch 9/50, Loss: 3203.7256236970425\n",
      "Epoch 10/50, Loss: 3201.2478472590446\n",
      "Epoch 11/50, Loss: 3198.1952142715454\n",
      "Epoch 12/50, Loss: 3193.772346884012\n",
      "Epoch 13/50, Loss: 3193.0757144391537\n",
      "Epoch 14/50, Loss: 3191.648870229721\n",
      "Epoch 15/50, Loss: 3187.7114550173283\n",
      "Epoch 16/50, Loss: 3187.3979715406895\n",
      "Epoch 17/50, Loss: 3182.9758153259754\n",
      "Epoch 18/50, Loss: 3178.705210506916\n",
      "Epoch 19/50, Loss: 3173.8695701658726\n",
      "Epoch 20/50, Loss: 3170.260470122099\n",
      "Epoch 21/50, Loss: 3167.7650876045227\n",
      "Epoch 22/50, Loss: 3167.2493092119694\n",
      "Epoch 23/50, Loss: 3166.244423389435\n",
      "Epoch 24/50, Loss: 3165.200751185417\n",
      "Epoch 25/50, Loss: 3164.42650526762\n",
      "Epoch 26/50, Loss: 3163.0863120555878\n",
      "Epoch 27/50, Loss: 3161.6579765081406\n",
      "Epoch 28/50, Loss: 3163.630938768387\n",
      "Epoch 29/50, Loss: 3160.0418220460415\n",
      "Epoch 30/50, Loss: 3160.242656081915\n",
      "Epoch 31/50, Loss: 3159.611973762512\n",
      "Epoch 32/50, Loss: 3159.11829534173\n",
      "Epoch 33/50, Loss: 3158.110841333866\n",
      "Epoch 34/50, Loss: 3158.4703287780285\n",
      "Epoch 35/50, Loss: 3157.096361517906\n",
      "Epoch 36/50, Loss: 3157.5884641110897\n",
      "Epoch 37/50, Loss: 3157.8159296512604\n",
      "Epoch 38/50, Loss: 3156.6895722448826\n",
      "Epoch 39/50, Loss: 3155.050557643175\n",
      "Epoch 40/50, Loss: 3155.0573605298996\n",
      "Epoch 41/50, Loss: 3155.527297049761\n",
      "Epoch 42/50, Loss: 3154.067647755146\n",
      "Epoch 43/50, Loss: 3153.7589001357555\n",
      "Epoch 44/50, Loss: 3152.711119055748\n",
      "Epoch 45/50, Loss: 3151.9855244755745\n",
      "Epoch 46/50, Loss: 3152.0841243863106\n",
      "Epoch 47/50, Loss: 3152.737410336733\n",
      "Epoch 48/50, Loss: 3152.8141631782055\n",
      "Epoch 49/50, Loss: 3152.5579888522625\n",
      "Epoch 50/50, Loss: 3150.7776100337505\n",
      "Accuracy: 0.6361809045226131\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat = []\n",
    "X_test_concat = []\n",
    "\n",
    "for review in X_train_w2v_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor = torch.tensor(X_train_concat, dtype=torch.float32)\n",
    "X_test_concat_tensor = torch.tensor(X_test_concat, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat = TensorDataset(X_train_concat_tensor, y_train_tensor)\n",
    "test_dataset_concat = TensorDataset(X_test_concat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_concat, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_concat, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Assuming 2 classes for sentiment analysis\n",
    "model_concat = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat, test_loader_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the First 10 Word2Vec vectors on Binary Classification for the Word2Vec Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3332.658839702606\n",
      "Epoch 2/50, Loss: 3261.719752550125\n",
      "Epoch 3/50, Loss: 3233.302696377039\n",
      "Epoch 4/50, Loss: 3221.9010125100613\n",
      "Epoch 5/50, Loss: 3212.4864708781242\n",
      "Epoch 6/50, Loss: 3204.434473723173\n",
      "Epoch 7/50, Loss: 3199.1797445118427\n",
      "Epoch 8/50, Loss: 3192.838485687971\n",
      "Epoch 9/50, Loss: 3188.5063311755657\n",
      "Epoch 10/50, Loss: 3183.40127363801\n",
      "Epoch 11/50, Loss: 3181.311291396618\n",
      "Epoch 12/50, Loss: 3177.702940106392\n",
      "Epoch 13/50, Loss: 3174.968383014202\n",
      "Epoch 14/50, Loss: 3173.754948168993\n",
      "Epoch 15/50, Loss: 3170.6849725842476\n",
      "Epoch 16/50, Loss: 3169.7179051041603\n",
      "Epoch 17/50, Loss: 3167.712255001068\n",
      "Epoch 18/50, Loss: 3165.444971472025\n",
      "Epoch 19/50, Loss: 3165.503632068634\n",
      "Epoch 20/50, Loss: 3164.026943206787\n",
      "Epoch 21/50, Loss: 3164.1536640822887\n",
      "Epoch 22/50, Loss: 3160.7471270263195\n",
      "Epoch 23/50, Loss: 3161.133596032858\n",
      "Epoch 24/50, Loss: 3159.1496563851833\n",
      "Epoch 25/50, Loss: 3156.0669295191765\n",
      "Epoch 26/50, Loss: 3155.8342953920364\n",
      "Epoch 27/50, Loss: 3153.715820401907\n",
      "Epoch 28/50, Loss: 3153.0616396069527\n",
      "Epoch 29/50, Loss: 3148.886031806469\n",
      "Epoch 30/50, Loss: 3148.472751110792\n",
      "Epoch 31/50, Loss: 3149.0304601490498\n",
      "Epoch 32/50, Loss: 3147.3468025922775\n",
      "Epoch 33/50, Loss: 3145.7648064792156\n",
      "Epoch 34/50, Loss: 3146.131478726864\n",
      "Epoch 35/50, Loss: 3145.435020685196\n",
      "Epoch 36/50, Loss: 3144.3417633771896\n",
      "Epoch 37/50, Loss: 3144.498016536236\n",
      "Epoch 38/50, Loss: 3142.430243074894\n",
      "Epoch 39/50, Loss: 3141.3271178901196\n",
      "Epoch 40/50, Loss: 3141.982347160578\n",
      "Epoch 41/50, Loss: 3139.948239952326\n",
      "Epoch 42/50, Loss: 3139.2576711177826\n",
      "Epoch 43/50, Loss: 3138.9660435020924\n",
      "Epoch 44/50, Loss: 3138.8182439506054\n",
      "Epoch 45/50, Loss: 3138.0010228455067\n",
      "Epoch 46/50, Loss: 3138.1844082176685\n",
      "Epoch 47/50, Loss: 3137.8724453151226\n",
      "Epoch 48/50, Loss: 3135.1284770965576\n",
      "Epoch 49/50, Loss: 3135.2824594676495\n",
      "Epoch 50/50, Loss: 3134.44172796607\n",
      "Accuracy: 0.6341206030150753\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the first 10 Word2Vec vectors for each review\n",
    "X_train_concat = []\n",
    "X_test_concat = []\n",
    "\n",
    "for review in X_train_w2v_own_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_train_concat.append(concatenated_vector)\n",
    "\n",
    "for review in X_test_w2v_own_binary:\n",
    "    review_reshaped = review.reshape(1, -1)  # Reshape to ensure it's a 2D array\n",
    "    concatenated_vector = np.concatenate(review_reshaped[:10], axis=0)\n",
    "    X_test_concat.append(concatenated_vector)\n",
    "\n",
    "# Convert the concatenated features into PyTorch tensors\n",
    "X_train_concat_tensor = torch.tensor(X_train_concat, dtype=torch.float32)\n",
    "X_test_concat_tensor = torch.tensor(X_test_concat, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to ternary format\n",
    "y_train_tensor = torch.tensor(y_train_binary, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_binary, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_concat = TensorDataset(X_train_concat_tensor, y_train_tensor)\n",
    "test_dataset_concat = TensorDataset(X_test_concat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_concat = DataLoader(train_dataset_concat, batch_size=32, shuffle=True)\n",
    "test_loader_concat = DataLoader(test_dataset_concat, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train_concat_tensor.shape[1]  # Size of input features\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 10\n",
    "output_size = 2  # Assuming 2 classes for sentiment analysis\n",
    "model_concat = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_concat.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train(model_concat, train_loader_concat, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model_concat, test_loader_concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
